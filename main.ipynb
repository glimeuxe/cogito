{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cogito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "- `m` is # of data points.\n",
    "- `n` is # of features.\n",
    "- `X` is `m`-by-`n` matrix representing features.\n",
    "- `y` is `m`-by-1 vector representing labels.\n",
    "- `w` is `n`-by-1 vector representing weights.\n",
    "- `b` is scalar representing bias.\n",
    "- `dw` is loss gradient w.r.t. `w`.\n",
    "- `db` is loss gradient w.r.t. `b`.\n",
    "\n",
    "### logreg(τ,α,G,β,L,λ)\n",
    "1. `τ` is # of epochs: non-negative integer.\n",
    "2. `α` is learning rate: float in range ${[0,1]}$.\n",
    "3. `G` is gradient descent type: `\"full-batch\"`, `\"mini-batch\"`, xor `\"stochastic\"`.\n",
    "4. `β` is batch size: non-negative integer xor `None`.\n",
    "5. `L` is regularisation type: `\"L2\"`, `\"L1\"`, xor `None`.\n",
    "6. `λ` is regularisation degree: real in range ${[0,\\infin)}$.\n",
    "\n",
    "### pcaknn(ρ,ν)\n",
    "1. `ρ` is # of principal components: integer in range ${[0,n]}$.\n",
    "2. `ν` is variance ratio: float in range ${[0,1]}$.\n",
    "\n",
    "### svm(λ,K,°,γ,κ,τ,seed)\n",
    "1. `λ` is margin regularisation degree.\n",
    "2. `K` is kernel type: `\"linear\"`, `\"poly\"`, `\"rbf\"`, `\"sigmoid\"`, xor `\"precomputed\"`.\n",
    "3. `°` is polynomial kernel degree for polynomial kernel type: non-negative integer.\n",
    "4. `γ` is kernel coefficient for RBF, polynomial, xor sigmoid kernel type: `\"scale\"`, `\"auto\"`, xor float in range ${[0,1]}$.\n",
    "5. `κ` is independent term of kernel function with effect for polynomial xor sigmoid kernel type.\n",
    "6. `τ` is maximum # of iterations: non-negative integer xor `-1`.\n",
    "7. `seed` is pseudorandom random state: integer xor `None`.\n",
    "\n",
    "### gbdtree(α,η,γ,ψ,ℓ,Δ,δ,seed,θ)\n",
    "1. `α` is learning rate: float in range ${[0,1]}$.\n",
    "2. `η` is # of estimators: non-negative integer.\n",
    "3. `γ` is fraction of samples used to fit individual base learners: float in range ${[0,1]}$.\n",
    "4. `ψ` is minimum # of samples needed to split internal node: non-negative integer.\n",
    "5. `ℓ` is minimum # of samples needed at leaf node: non-negative integer.\n",
    "6. `Δ` is maximum tree depth: non-negative integer xor `None`.\n",
    "7. `δ` is minimum impurity decrease: float in range ${[0,1]}$.\n",
    "8. `seed` is pseudorandom random state: integer xor `None`.\n",
    "9. `θ` is maximum # of leaf nodes: non-negative integer xor `None`.\n",
    "\n",
    "#### hgbdtree(α,τ,θ,Δ,ℓ,seed)\n",
    "1. `α` is learning rate: float in range ${[0,1]}$.\n",
    "2. `τ` is maximum # of iterations: non-negative integer.\n",
    "3. `θ` is maximum # of leaf nodes: non-negative integer xor `None`.\n",
    "4. `Δ` is maximum tree depth: non-negative integer xor `None`.\n",
    "5. `ℓ` is minimum # of samples needed at leaf node: non-negative integer xor `None`.\n",
    "6. `seed` is pseudorandom random state: integer xor `None`.\n",
    "\n",
    "#### rforest(η,C,Δ,ψ,ℓ,θ,δ,B,seed)\n",
    "1. `η` is # of estimators: non-negative integer.\n",
    "2. `C` is criterion to measure split quality: `\"entropy\"`, `\"log_loss\"`, xor `\"gini\"`.\n",
    "3. `Δ` is maximum tree depth: non-negative integer xor `None`.\n",
    "4. `ψ` is minimum # of samples needed to split internal node: non-negative integer.\n",
    "5. `ℓ` is minimum # of samples needed at leaf node: non-negative integer.\n",
    "6. `θ` is maximum # of leaf nodes: non-negative integer xor `None`.\n",
    "7. `δ` is minimum impurity decrease: float in range ${[0,1]}$.\n",
    "8. `B` is whether bootstrapping is used: `False` xor `True`.\n",
    "9. `seed` is pseudorandom random state: integer xor `None`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- numpy, pandas, and sklearn (scikit-learn) must be installed and available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for operating system functions.\n",
    "import os\n",
    "# Import numpy for mathematical computation.\n",
    "import numpy as np\n",
    "# Import pandas for data manipulation.\n",
    "import pandas as pd\n",
    "# Import datetime for local time retrieval.\n",
    "from datetime import datetime\n",
    "# Import logging for file logging.\n",
    "import logging\n",
    "# Import time for duration measurement.\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Import PCA and KNeighborsClassifier for pcaknn.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Import RandomForestClassifier for rforest.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import SVC for svm model.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Import GradientBoostingClassifier for gbdtree.\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Import HistGradientBoostingClassifier for hgbdtree.\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Import StackingClassifier, LogisticRegression, BaseEstimator, and ClassifierMixin for stack.\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Initialise file logging configuration.\n",
    "logging.basicConfig(\n",
    "\tfilename=\"main.log\",\n",
    "\tlevel=logging.INFO,\n",
    "\tformat=\"%(asctime)s - %(message)s\",\n",
    "\tdatefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Load train and test datasets.\n",
    "S_train = pd.read_csv(\"train.csv\")\n",
    "S_train_tfidf = pd.read_csv(\"train_tfidf_features.csv\")\n",
    "S_test = pd.read_csv(\"test.csv\")\n",
    "S_test_tfidf = pd.read_csv(\"test_tfidf_features.csv\")\n",
    "\n",
    "# Extract train features, train labels, and test features.\n",
    "X_train = S_train_tfidf.iloc[:, 2:].values\n",
    "y_train = S_train[\"label\"].values.reshape(-1, 1)\n",
    "X_test = S_test_tfidf.iloc[:, 1:].values\n",
    "\n",
    "# Define generic functions.\n",
    "# Return σ(z) for some z.\n",
    "def σ(z): return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Return cross-entropy loss for some y and ŷ.\n",
    "# def loss(y, ŷ): return (-1/(y.shape[0])) * np.sum(y * np.log(ŷ) + (1 - y) * np.log(1 - ŷ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Logistic Regression Model (`logreg`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return dw and db, for some X, y, ŷ, w, λ, and L.\n",
    "def gradients_logreg(X, y, ŷ, w, L=None, λ=0):\n",
    "\tm, _ = X.shape\n",
    "\tdw = 1/m * np.dot(X.T, (ŷ - y))\n",
    "\tdb = 1/m * np.sum(ŷ - y)\n",
    "\tif L == \"L2\":\n",
    "\t\tdw += λ * w / m\n",
    "\telif L == \"L1\":\n",
    "\t\tdw += λ * np.sign(w) / m\n",
    "\treturn dw, db\n",
    "\n",
    "# Return w and b from gradient descent on X and y, for some β, τ, α, λ, L, and G.\n",
    "def train_logreg(τ=1000, α=0.1, G=\"mini-batch\", β=128, L=None, λ=0):\n",
    "\tm, n = X_train.shape\n",
    "\tw, b = np.zeros((n, 1)), 0\n",
    "\tfor epoch in range(τ):\n",
    "\t\tif G == \"full-batch\":\n",
    "\t\t\tX_batch, y_batch = X_train, y_train\n",
    "\t\t\tŷ = σ(np.dot(X_batch, w) + b)\n",
    "\t\t\tdw, db = gradients_logreg(X_batch, y_batch, ŷ, w, L, λ)\n",
    "\t\t\tw, b = w - α*dw, b - α*db\n",
    "\t\telif G == \"mini-batch\":\n",
    "\t\t\tfor i in range(0, m, β):\n",
    "\t\t\t\tX_batch, y_batch = X_train[i:i+β], y_train[i:i+β]\n",
    "\t\t\t\tŷ = σ(np.dot(X_batch, w) + b)\n",
    "\t\t\t\tdw, db = gradients_logreg(X_batch, y_batch, ŷ, w, L, λ)\n",
    "\t\t\t\tw, b = w - α*dw, b - α*db\n",
    "\t\telif G == \"stochastic\":\n",
    "\t\t\tfor i in range(m):\n",
    "\t\t\t\tX_batch, y_batch = X_train[i:i+1], y_train[i:i+1]\n",
    "\t\t\t\tŷ = σ(np.dot(X_batch, w) + b)\n",
    "\t\t\t\tdw, db = gradients_logreg(X_batch, y_batch, ŷ, w, L, λ)\n",
    "\t\t\t\tw, b = w - α*dw, b - α*db\n",
    "\treturn w, b\n",
    "\n",
    "# Return array of predictions, where each prediction is 1 if corresponding ŷ entry > 0.5, and 0 otherwise.\n",
    "def predict_logreg(w, b):\n",
    "\t# Compute ŷ = σ(w ⋅ X + b).\n",
    "\tŷ = σ(np.dot(X_test, w) + b)\n",
    "\treturn [1 if p > 0.5 else 0 for p in ŷ]\n",
    "\n",
    "# Train model, make model predictions, and save model predictions to CSV file, for some β, τ, α, λ, L, and G.\n",
    "def generate_predictions_logreg(τ=1000, α=0.1, G=\"mini-batch\", β=128, L=None, λ=0):\n",
    "\tstart_time = time.time()\n",
    "\tw, b = train_logreg(τ, α, G, β, L, λ)\n",
    "\tos.makedirs(\"./predictions/logreg/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/logreg/\", f\"G={G},β={β},L={L},λ={λ},τ={τ},α={α}.csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predict_logreg(w, b)\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# generate_predictions_logreg(10, 0.1, \"stochastic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: PCA-KNN Model (`pcaknn`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return PCA-transformed train and test featuresets, for either some ν or ρ.\n",
    "def apply_pca(mode_value, mode):\n",
    "\t# Validate mode and mode value.\n",
    "\tif mode not in [\"ν\", \"ρ\"] or (mode == \"ν\" and not (0 < mode_value <= 1)) or (mode == \"ρ\" and not (mode_value > 1 and isinstance(mode_value, int))):\n",
    "\t\traise ValueError(\"Mode must either be ν with value between 0 and 1, or ρ with mode_value as an integer greater than 1.\")\n",
    "\t# Instantiate PCA and StandardScaler objects.\n",
    "\tpca = PCA(n_components=mode_value)\n",
    "\tscaler = StandardScaler()\n",
    "\t# PCA-transform train and test featuresets.\n",
    "\tX_train_pca = pca.fit_transform(scaler.fit_transform(X_train))\n",
    "\tX_test_pca = pca.transform(scaler.transform(X_test))\n",
    "\t# Return PCA-transformed featuresets and # of components.\n",
    "\treturn X_train_pca, X_test_pca, pca.n_components_\n",
    "\n",
    "# Return KNN predictions for PCA-transformed train and test datasets.\n",
    "def train_and_predict_knn(X_train_pca, y_train, X_test_pca):\n",
    "\t# Instantiate KNeighborsClassifier object.\n",
    "\tknn = KNeighborsClassifier(n_neighbors=2)\n",
    "\tknn.fit(X_train_pca, y_train)\n",
    "\treturn knn.predict(X_test_pca)\n",
    "\n",
    "# Train model, make model predictions, and save model predictions to CSV file, for some ν or ρ.\n",
    "def generate_predictions_pcaknn(mode_value, mode=\"ρ\"):\n",
    "\t# Validate mode and mode value.\n",
    "\tif mode not in [\"ν\", \"ρ\"] or (mode == \"ν\" and not (0 < mode_value <= 1)) or (mode == \"ρ\" and not (mode_value > 1 and isinstance(mode_value, int))):\n",
    "\t\traise ValueError(\"Mode must either be ν with value between 0 and 1, or ρ with mode_value as an integer greater than 1.\")\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tX_train_pca, X_test_pca, n_components = apply_pca(mode_value, mode)\n",
    "\tos.makedirs(\"./predictions/pcaknn/\", exist_ok=True)\n",
    "\tif mode == \"ν\":\n",
    "\t\tfile_name = os.path.join(\"./predictions/pcaknn/\", f\"pcaknn(ν={mode_value},ρ={n_components}).csv\")\n",
    "\telse:\n",
    "\t\tfile_name = os.path.join(\"./predictions/pcaknn/\", f\"pcaknn(ρ={n_components}).csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": train_and_predict_knn(X_train_pca, y_train, X_test_pca)\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# generate_predictions_pcaknn(mode_value=5000, mode=\"ρ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model (`rforest`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RandomForest classifier, make predictions, and save predictions to CSV file.\n",
    "def generate_predictions_rforest(η=100, C=\"gini\", Δ=None, ψ=2, ℓ=1, θ=None, δ=0.0, B=True, seed=None):\n",
    "\tstart_time = time.time()\n",
    "\trf = RandomForestClassifier(\n",
    "\t\tn_estimators=η,\n",
    "\t\tcriterion=C,\n",
    "\t\tmax_depth=Δ,\n",
    "\t\tmin_samples_split=ψ,\n",
    "\t\tmin_samples_leaf=ℓ,\n",
    "\t\tmax_leaf_nodes=θ,\n",
    "\t\tmin_impurity_decrease=δ,\n",
    "\t\tbootstrap=B,\n",
    "\t\trandom_state=seed\n",
    "\t)\n",
    "\trf.fit(X_train, y_train.ravel())\n",
    "\tpredictions = rf.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/rforest/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/rforest/\", f\"𝒹={𝒹},𝓈={𝓈},ℓ={ℓ},η={η}.csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# generate_predictions_rforest(η=100, C=\"gini\", Δ=None, ψ=2, ℓ=1, θ=None, δ=0.0, B=True, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model (`svm`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM classifier, make predictions, and save predictions to CSV file.\n",
    "def generate_predictions_svm(λ=1.0, K=\"rbf\", °=3, γ=\"scale\", κ=0.0, τ=-1, seed=None):\n",
    "\tstart_time = time.time()\n",
    "\tsvm_clf = SVC(\n",
    "\t\tC=λ,\n",
    "\t\tkernel=K,\n",
    "\t\tdegree=°,\n",
    "\t\tgamma=γ,\n",
    "\t\tcoef0=κ,\n",
    "\t\tmax_iter=τ,\n",
    "\t\trandom_state=seed\n",
    "\t)\n",
    "\tsvm_clf.fit(X_train, y_train.ravel())\n",
    "\tpredictions = svm_clf.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/svm/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/svm/\", f\"λ={λ},K={K},°={°},γ={γ},κ={κ},τ={τ},seed={seed}.csv\")\n",
    "\tsubmission = pd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t})\n",
    "\tsubmission.to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# generate_predictions_svm(λ=1.0, K=\"rbf\", °=3, γ=\"scale\", κ=0.0, τ=-1, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Decision Tree Model (`gbdtree`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_gbdtree(α=0.1, η=100, γ=1.0, ψ=2, ℓ=1, Δ=3, δ=0.0, seed=None, θ=None):\n",
    "\tstart_time = time.time()\n",
    "\tgbc = GradientBoostingClassifier(\n",
    "\t\tlearning_rate=α,\n",
    "\t\tn_estimators=η,\n",
    "\t\tsubsample=γ,\n",
    "\t\tmin_samples_split=ψ,\n",
    "\t\tmin_samples_leaf=ℓ,\n",
    "\t\tmax_depth=Δ,\n",
    "\t\tmin_impurity_decrease=δ,\n",
    "\t\trandom_state=seed,\n",
    "\t\tmax_leaf_nodes=θ\n",
    "\t)\n",
    "\tgbc.fit(X_train, y_train)\n",
    "\tpredictions = gbc.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/gbdtree/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/gbdtree/\", f\"α={α},η={η},γ={γ},ψ={ψ},ℓ={ℓ},Δ={Δ},δ={δ},seed={seed},θ={θ}.csv\")\n",
    "\tsubmission = pd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t})\n",
    "\tsubmission.to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "generate_predictions_gbdtree(α=0.1, η=100, γ=1.0, ψ=2, ℓ=1, Δ=3, δ=0.0, seed=None, θ=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram Gradient Boosted Decision Tree Model (`hgbdtree`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_hgbdtree(α=0.1, τ=100, θ=31, Δ=None, ℓ=20, seed=None):\n",
    "\tstart_time = time.time()\n",
    "\thgbc = HistGradientBoostingClassifier(\n",
    "\t\tlearning_rate=α,\n",
    "\t\tmax_iter=τ,\n",
    "\t\tmax_leaf_nodes=θ,\n",
    "\t\tmax_depth=Δ,\n",
    "\t\tmin_samples_leaf=ℓ,\n",
    "\t\trandom_state=seed\n",
    "\t)\n",
    "\thgbc.fit(X_train, y_train)\n",
    "\tpredictions = hgbc.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/hgbdtree/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/hgbdtree/\", f\"α={α},τ={τ},θ={θ},Δ={Δ},ℓ={ℓ},seed={seed}.csv\")\n",
    "\tsubmission = pd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t})\n",
    "\tsubmission.to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# generate_predictions_hgbdtree(α=0.1, τ=100, θ=31, Δ=None, ℓ=20, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wrapper classes for original (i.e. non-imported) models.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
