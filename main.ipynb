{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cogito\n",
    "Œ±Œ≤œÑœÅŒΩùíπùìà‚ÑìŒ∑ŒªŒµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Definitions\n",
    "- Let `X` be a `m`-by-`n` matrix of input data, and `y` be a `m`-by-1 vector of output data, where `m` is the # of data points, and `n` is the # of features, so that `X` represents the features and `y` represents the labels.\n",
    "- Let `w` be a `n`-by-1 vector of weights, and `b` be a scalar of bias.\n",
    "- Let `dw` be the loss gradient w.r.t. `w`, and `db` be the loss gradient w.r.t. `b`, where for least squares we have ${dw=\\frac{1}{m}\\sum_{i=1}^{m}(yÃÇ^{(i)} - y^{(i)})x^{(i)}+\\frac{\\lambda w}{m}}$ and ${db = \\frac{1}{m}\\sum_{i=1}^{m}(yÃÇ^{(i)} - y^{(i)})}$.\n",
    "- Let `Œ≤` be the batch size.\n",
    "- Let `œÑ` be the # of epochs.\n",
    "- Let `Œ±` be the learning rate.\n",
    "- Let `œÅ` be the # of principal components (where `n` is the maximum possible).\n",
    "- Let `ŒΩ` be the variance ratio.\n",
    "- Let `ùíπ` be the maximum depth.\n",
    "- Let `ùìà` be the minimum number of samples to split an internal node.\n",
    "- Let `‚Ñì` be the minimum number of samples at a leaf node.\n",
    "- Let `Œ∑` be the number of estimators.\n",
    "- Let `Œª` be either the L2 regularisation parameter (in context of logistic regression), or the C parameter (in the context of SVM).\n",
    "- Let `Œµ` be a small positive constant.\n",
    "\n",
    "### Naming Conventions\n",
    "1. Dataset is treated as a single word.\n",
    "2. Featureset refers to the features of a dataset.\n",
    "3. Labelset refers to the labels of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- numpy, pandas, and sklearn (scikit-learn) must be installed and available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for operating system functions.\n",
    "import os\n",
    "# Import numpy for mathematical computation.\n",
    "import numpy as np\n",
    "# Import pandas for data manipulation.\n",
    "import pandas as pd\n",
    "# Import datetime for local time retrieval.\n",
    "from datetime import datetime\n",
    "# Import logging for file logging.\n",
    "import logging\n",
    "# Import time for duration measurement.\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Import PCA and KNeighborsClassifier for PCA-KNN model.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Import DecisionTreeClassifier for dtree model.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Import RandomForestClassifier for rforest model.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Import SVC for SVM model.\n",
    "from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# Import ... for stack model.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Initialise file logging configuration.\n",
    "logging.basicConfig(filename=\"main.log\", level=logging.INFO, format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Load train and test datasets.\n",
    "S_train = pd.read_csv(\"train.csv\")\n",
    "S_train_tfidf = pd.read_csv(\"train_tfidf_features.csv\")\n",
    "S_test = pd.read_csv(\"test.csv\")\n",
    "S_test_tfidf = pd.read_csv(\"test_tfidf_features.csv\")\n",
    "\n",
    "# Extract train features, train labels, and test features.\n",
    "X_train = S_train_tfidf.iloc[:, 2:].values\n",
    "y_train = S_train[\"label\"].values.reshape(-1, 1)\n",
    "X_test = S_test_tfidf.iloc[:, 1:].values\n",
    "\n",
    "# Define generic functions.\n",
    "# Return œÉ(z) for some z.\n",
    "def œÉ(z): return 1 / (1 + np.exp(-z))\n",
    "# Return cross-entropy loss for some y and yÃÇ.\n",
    "# def loss(y, yÃÇ): return (-1/(y.shape[0])) * np.sum(y * np.log(yÃÇ) + (1 - y) * np.log(1 - yÃÇ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Mini-Batch Gradient Descent Logistic Regression Model (mgdlogreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return dw and db, for some X, y, and yÃÇ.\n",
    "def gradients_mgdlogreg(X, y, yÃÇ):\n",
    "\tm, _ = X.shape\n",
    "\tdw = 1/m * np.dot(X.T, (yÃÇ - y))\n",
    "\tdb = 1/m * np.sum(yÃÇ - y)\n",
    "\treturn dw, db\n",
    "\n",
    "# Return w and b from mini-batch gradient descent on X and y, for batch size Œ≤, epochs œÑ, and learning rate Œ±.\n",
    "def train_mgdlogreg(Œ≤, œÑ, Œ±):\n",
    "\tm, n = X_train.shape\n",
    "\t# Initialise w to 0 vector and b to 0.\n",
    "\tw, b = np.zeros((n, 1)), 0\n",
    "\t# For each epoch:\n",
    "\tfor epoch in range(œÑ):\n",
    "\t\t# For every batch within epoch:\n",
    "\t\tfor i in range(0, m, Œ≤):\n",
    "\t\t\tX_batch, y_batch = X_train[i:i+Œ≤], y_train[i:i+Œ≤]\n",
    "\t\t\tyÃÇ = œÉ(np.dot(X_batch, w) + b)\n",
    "\t\t\tdw, db = gradients_mgdlogreg(X_batch, y_batch, yÃÇ)\n",
    "\t\t\tw, b = w - Œ±*dw, b - Œ±*db\n",
    "\treturn w, b\n",
    "\n",
    "# Return array of predictions, where each prediction is 1 if corresponding yÃÇ entry > 0.5, and 0 otherwise.\n",
    "def predict_mgdlogreg(w, b):\n",
    "\t# Compute yÃÇ = œÉ(w‚ãÖX + b).\n",
    "\tyÃÇ = œÉ(np.dot(X_test, w) + b)\n",
    "\treturn [1 if p > 0.5 else 0 for p in yÃÇ]\n",
    "\n",
    "# Train model, make model predictions, and save model predictions to CSV file, for some Œ≤, œÑ, and Œ±.\n",
    "def generate_predictions_mgdlogreg(Œ≤, œÑ, Œ±):\n",
    "\tstart_time = time.time()\n",
    "\tw, b = train_mgdlogreg(Œ≤, œÑ, Œ±)\n",
    "\tos.makedirs(\"./predictions/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/\", f\"mgdlogreg(Œ≤={Œ≤},œÑ={œÑ},Œ±={Œ±}).csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predict_mgdlogreg(w, b)\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# Example usage:\n",
    "# generate_predictions_mgdlogreg(128, 1200, 0.084)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: PCA-KNN Model (pcaknn)\n",
    "### Usage Examples\n",
    "1. `generate_predictions_pcaknn(X_train, y_train, X_test, 0.95, \"ŒΩ\")`\n",
    "2. `generate_predictions_pcaknn(X_train, y_train, X_test, 2000, \"œÅ\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return PCA-transformed train and test featuresets, for either some ŒΩ or œÅ.\n",
    "def apply_pca(mode_value, mode):\n",
    "\t# Validate mode and mode value.\n",
    "\tif mode not in [\"ŒΩ\", \"œÅ\"] or (mode == \"ŒΩ\" and not (0 < mode_value <= 1)) or (mode == \"œÅ\" and not (mode_value > 1 and isinstance(mode_value, int))):\n",
    "\t\traise ValueError(\"Mode must either be ŒΩ with value between 0 and 1, or œÅ with mode_value as an integer greater than 1.\")\n",
    "\t# Instantiate PCA and StandardScaler objects.\n",
    "\tpca = PCA(n_components=mode_value)\n",
    "\tscaler = StandardScaler()\n",
    "\t# PCA-transform train and test featuresets.\n",
    "\tX_train_pca = pca.fit_transform(scaler.fit_transform(X_train))\n",
    "\tX_test_pca = pca.transform(scaler.transform(X_test))\n",
    "\t# Return PCA-transformed featuresets and # of components.\n",
    "\treturn X_train_pca, X_test_pca, pca.n_components_\n",
    "\n",
    "# Return KNN predictions for PCA-transformed train and test datasets.\n",
    "def train_and_predict_knn(X_train_pca, y_train, X_test_pca):\n",
    "\t# Instantiate KNeighborsClassifier object.\n",
    "\tknn = KNeighborsClassifier(n_neighbors=2)\n",
    "\tknn.fit(X_train_pca, y_train)\n",
    "\treturn knn.predict(X_test_pca)\n",
    "\n",
    "# Train model, make model predictions, and save model predictions to CSV file, for some ŒΩ or œÅ.\n",
    "def generate_predictions_pcaknn(mode_value, mode):\n",
    "\t# Validate mode and mode value.\n",
    "\tif mode not in [\"ŒΩ\", \"œÅ\"] or (mode == \"ŒΩ\" and not (0 < mode_value <= 1)) or (mode == \"œÅ\" and not (mode_value > 1 and isinstance(mode_value, int))):\n",
    "\t\traise ValueError(\"Mode must either be ŒΩ with value between 0 and 1, or œÅ with mode_value as an integer greater than 1.\")\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tX_train_pca, X_test_pca, n_components = apply_pca(mode_value, mode)\n",
    "\tos.makedirs(\"./predictions/pcaknn/\", exist_ok=True)\n",
    "\tif mode == \"ŒΩ\":\n",
    "\t\tfile_name = os.path.join(\"./predictions/pcaknn/\", f\"pcaknn(ŒΩ={mode_value},œÅ={n_components}).csv\")\n",
    "\telse:\n",
    "\t\tfile_name = os.path.join(\"./predictions/pcaknn/\", f\"pcaknn(œÅ={n_components}).csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": train_and_predict_knn(X_train_pca, y_train, X_test_pca)\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# generate_predictions_pcaknn(5000, \"œÅ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Other Models\n",
    "### Decision Tree Model (dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_dtree(ùíπ, ùìà, ‚Ñì):\n",
    "\tstart_time = time.time()\n",
    "\tmodel = DecisionTreeClassifier(max_depth=ùíπ, min_samples_split=ùìà, min_samples_leaf=‚Ñì)\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\tpredictions = model.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/dtree/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/dtree/\", f\"ùíπ={ùíπ},ùìà={ùìà},‚Ñì={‚Ñì}.csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# generate_predictions_dtree(ùíπ=50, ùìà=10, ‚Ñì=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model (rforest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RandomForest classifier, make predictions, and save predictions to CSV file.\n",
    "def train_and_predict_rforest(ùíπ, ùìà, ‚Ñì, Œ∑):\n",
    "\tstart_time = time.time()\n",
    "\trf = RandomForestClassifier(max_depth=ùíπ, min_samples_split=ùìà, min_samples_leaf=‚Ñì, n_estimators=Œ∑, random_state=42)\n",
    "\trf.fit(X_train, y_train.ravel())\n",
    "\tpredictions = rf.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/rforest/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/rforest/\", f\"ùíπ={ùíπ},ùìà={ùìà},‚Ñì={‚Ñì},Œ∑={Œ∑}.csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# train_and_predict_rforest(ùíπ=10000, ùìà=10, ‚Ñì=2, Œ∑=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Adagrad Mini-Batch Gradient Descent Logistic Regression Model (l2amgdlogreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return dw and db, for some X, y, yÃÇ, and Œª.\n",
    "def gradients_l2amgdlogreg(X, y, yÃÇ, w, Œª):\n",
    "\tm, _ = X.shape\n",
    "\tdw = 1/m * np.dot(X.T, (yÃÇ - y)) + (Œª/m) * w\n",
    "\tdb = 1/m * np.sum(yÃÇ - y)\n",
    "\treturn dw, db\n",
    "\n",
    "# Return w and b from L2 adagrad mini-batch gradient descent on X_train and y_train, for batch size Œ≤, epochs œÑ, initial learning rate Œ±, and regularisation Œª.\n",
    "def train_l2amgdlogreg(Œ≤, œÑ, Œ±, Œª=0, Œµ=1e-8):\n",
    "\tm, n = X_train.shape\n",
    "\t# Initialise w to 0 vector and b to 0.\n",
    "\tw, b = np.zeros((n, 1)), 0\n",
    "\t# Initialise gradient accumulation variables for Adagrad.\n",
    "\tsdw, sdb = np.zeros((n, 1)), 0\n",
    "\t# For each epoch:\n",
    "\tfor epoch in range(œÑ):\n",
    "\t\t# For every batch within epoch:\n",
    "\t\tfor i in range(0, m, Œ≤):\n",
    "\t\t\tX_batch, y_batch = X_train[i:i+Œ≤], y_train[i:i+Œ≤]\n",
    "\t\t\tyÃÇ = œÉ(np.dot(X_batch, w) + b)\n",
    "\t\t\tdw, db = gradients_l2amgdlogreg(X_batch, y_batch, yÃÇ, w, Œª)\n",
    "\t\t\tsdw += dw ** 2\n",
    "\t\t\tsdb += db ** 2\n",
    "\t\t\tw -= (Œ± / (np.sqrt(sdw) + Œµ)) * dw\n",
    "\t\t\tb -= (Œ± / (np.sqrt(sdb) + Œµ)) * db\n",
    "\treturn w, b\n",
    "\n",
    "# Return array of predictions, where each prediction is 1 if corresponding yÃÇ entry > 0.5, and 0 otherwise.\n",
    "def predict_l2amgdlogreg(w, b):\n",
    "\t# Compute yÃÇ = œÉ(w‚ãÖX + b).\n",
    "\tyÃÇ = œÉ(np.dot(X_test, w) + b)\n",
    "\treturn [1 if p > 0.5 else 0 for p in yÃÇ]\n",
    "\n",
    "# Train model, make model predictions, and save model predictions to CSV file, for some Œ≤, œÑ, Œ±, and Œª.\n",
    "def generate_predictions_l2amgdlogreg(Œ≤, œÑ, Œ±, Œª=0):\n",
    "\tstart_time = time.time()\n",
    "\tw, b = train_l2amgdlogreg(Œ≤, œÑ, Œ±, Œª)\n",
    "\tos.makedirs(\"./predictions/l2amgdlogreg/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/l2amgdlogreg/\", f\"Œ≤={Œ≤},œÑ={œÑ},Œ±={Œ±},Œª={Œª}.csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predict_l2amgdlogreg(w, b)\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# generate_predictions_l2amgdlogreg(Œ≤=128, œÑ=2000, Œ±=0.0825, Œª=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM classifier, make predictions, and save predictions to CSV file.\n",
    "def generate_predictions_svm(Œ∫, Œª):\n",
    "\tstart_time = time.time()\n",
    "\tsvm_clf = SVC(kernel=Œ∫, C=Œª, random_state=42)\n",
    "\tsvm_clf.fit(X_train, y_train.ravel())\n",
    "\tpredictions = svm_clf.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/svm/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/svm/\", f\"Œ∫={Œ∫},Œª={Œª}.csv\")\n",
    "\tsubmission = pd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t})\n",
    "\tsubmission.to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# generate_predictions_svm(\"linear\", 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Ensemble Model (stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wrapper classes for original (i.e. non-imported) models.\n",
    "class CustomL2AMGDLogReg(BaseEstimator, ClassifierMixin):\n",
    "\tdef __init__(self, Œ≤=128, œÑ=2000, Œ±=0.0825, Œª=0.01):\n",
    "\t\tself.Œ≤ = Œ≤\n",
    "\t\tself.œÑ = œÑ\n",
    "\t\tself.Œ± = Œ±\n",
    "\t\tself.Œª = Œª\n",
    "\t\tself.w = None\n",
    "\t\tself.b = None\n",
    "\n",
    "\tdef fit(self, X, y):\n",
    "\t\tself.w, self.b = train_l2amgdlogreg(self.Œ≤, self.œÑ, self.Œ±, self.Œª)\n",
    "\t\treturn self\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\treturn predict_l2amgdlogreg(self.w, self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 78\u001b[0m\n\u001b[1;32m     68\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m generated in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     71\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2amgdlogreg\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŒ≤\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m128\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mœÑ\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2000\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŒ±\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0825\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŒª\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.01\u001b[39m},\n\u001b[1;32m     72\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mknn\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_neighbors\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŒª\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m}\n\u001b[1;32m     76\u001b[0m }\n\u001b[0;32m---> 78\u001b[0m generate_predictions_stack(hyperparameters)\n",
      "Cell \u001b[0;32mIn[25], line 50\u001b[0m, in \u001b[0;36mgenerate_predictions_stack\u001b[0;34m(hyperparameters)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_predictions_stack\u001b[39m(hyperparameters):\n\u001b[1;32m     49\u001b[0m \tstart_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 50\u001b[0m \tmodels \u001b[38;5;241m=\u001b[39m train_models(hyperparameters)\n\u001b[1;32m     51\u001b[0m \tstacking_clf \u001b[38;5;241m=\u001b[39m StackingClassifier(\n\u001b[1;32m     52\u001b[0m \t\testimators\u001b[38;5;241m=\u001b[39mmodels,\n\u001b[1;32m     53\u001b[0m \t\tfinal_estimator\u001b[38;5;241m=\u001b[39mLogisticRegression(),\n\u001b[1;32m     54\u001b[0m \t\tcv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     55\u001b[0m \t)\n\u001b[1;32m     56\u001b[0m \tstacking_clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train\u001b[38;5;241m.\u001b[39mravel())\n",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m, in \u001b[0;36mtrain_models\u001b[0;34m(hyperparameters)\u001b[0m\n\u001b[1;32m      2\u001b[0m models \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m l2amgdlogreg_clf \u001b[38;5;241m=\u001b[39m CustomL2AMGDLogReg(\n\u001b[1;32m      5\u001b[0m \tŒ≤\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2amgdlogreg\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŒ≤\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m \tœÑ\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2amgdlogreg\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mœÑ\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m \tŒ±\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2amgdlogreg\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŒ±\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m \tŒª\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2amgdlogreg\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŒª\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m l2amgdlogreg_clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train\u001b[38;5;241m.\u001b[39mravel())\n\u001b[1;32m     11\u001b[0m models\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2amgdlogreg\u001b[39m\u001b[38;5;124m\"\u001b[39m, l2amgdlogreg_clf))\n\u001b[1;32m     13\u001b[0m knn_clf \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(\n\u001b[1;32m     14\u001b[0m \tn_neighbors\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mknn\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_neighbors\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m )\n",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m, in \u001b[0;36mCustomL2AMGDLogReg.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m---> 12\u001b[0m \t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m train_l2amgdlogreg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mŒ≤, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mœÑ, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mŒ±, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mŒª)\n\u001b[1;32m     13\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[22], line 21\u001b[0m, in \u001b[0;36mtrain_l2amgdlogreg\u001b[0;34m(Œ≤, œÑ, Œ±, Œª, Œµ)\u001b[0m\n\u001b[1;32m     19\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_train[i:i\u001b[38;5;241m+\u001b[39mŒ≤], y_train[i:i\u001b[38;5;241m+\u001b[39mŒ≤]\n\u001b[1;32m     20\u001b[0m yÃÇ \u001b[38;5;241m=\u001b[39m œÉ(np\u001b[38;5;241m.\u001b[39mdot(X_batch, w) \u001b[38;5;241m+\u001b[39m b)\n\u001b[0;32m---> 21\u001b[0m dw, db \u001b[38;5;241m=\u001b[39m gradients_l2amgdlogreg(X_batch, y_batch, yÃÇ, w, Œª)\n\u001b[1;32m     22\u001b[0m sdw \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dw \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     23\u001b[0m sdb \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m db \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m, in \u001b[0;36mgradients_l2amgdlogreg\u001b[0;34m(X, y, ≈∑, w, Œª)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradients_l2amgdlogreg\u001b[39m(X, y, yÃÇ, w, Œª):\n\u001b[1;32m      3\u001b[0m \tm, _ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m----> 4\u001b[0m \tdw \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X\u001b[38;5;241m.\u001b[39mT, (yÃÇ \u001b[38;5;241m-\u001b[39m y)) \u001b[38;5;241m+\u001b[39m (Œª\u001b[38;5;241m/\u001b[39mm) \u001b[38;5;241m*\u001b[39m w\n\u001b[1;32m      5\u001b[0m \tdb \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(yÃÇ \u001b[38;5;241m-\u001b[39m y)\n\u001b[1;32m      6\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m dw, db\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_models(hyperparameters):\n",
    "\tmodels = []\n",
    "\n",
    "\tl2amgdlogreg_clf = CustomL2AMGDLogReg(\n",
    "\t\tŒ≤=hyperparameters[\"l2amgdlogreg\"][\"Œ≤\"],\n",
    "\t\tœÑ=hyperparameters[\"l2amgdlogreg\"][\"œÑ\"],\n",
    "\t\tŒ±=hyperparameters[\"l2amgdlogreg\"][\"Œ±\"],\n",
    "\t\tŒª=hyperparameters[\"l2amgdlogreg\"][\"Œª\"]\n",
    "\t)\n",
    "\tl2amgdlogreg_clf.fit(X_train, y_train.ravel())\n",
    "\tmodels.append((\"l2amgdlogreg\", l2amgdlogreg_clf))\n",
    "\n",
    "\tknn_clf = KNeighborsClassifier(\n",
    "\t\tn_neighbors=hyperparameters[\"knn\"][\"n_neighbors\"]\n",
    "\t)\n",
    "\tknn_clf.fit(X_train, y_train.ravel())\n",
    "\tmodels.append((\"knn\", knn_clf))\n",
    "\n",
    "\tdtree_clf = DecisionTreeClassifier(\n",
    "\t\trandom_state=hyperparameters[\"dtree\"][\"random_state\"],\n",
    "\t\tmax_depth=hyperparameters[\"dtree\"][\"ùíπ\"],\n",
    "\t\tmin_samples_split=hyperparameters[\"dtree\"][\"ùìà\"],\n",
    "\t\tmin_samples_leaf=hyperparameters[\"dtree\"][\"‚Ñì\"]\n",
    "\t)\n",
    "\tdtree_clf.fit(X_train, y_train.ravel())\n",
    "\tmodels.append((\"dtree\", dtree_clf))\n",
    "\n",
    "\trforest_clf = RandomForestClassifier(\n",
    "\t\trandom_state=hyperparameters[\"rforest\"][\"random_state\"],\n",
    "\t\tn_estimators=hyperparameters[\"rforest\"][\"Œ∑\"]\n",
    "\t)\n",
    "\trforest_clf.fit(X_train, y_train.ravel())\n",
    "\tmodels.append((\"rforest\", rforest_clf))\n",
    "\n",
    "\tsvm_clf = SVC(\n",
    "\t\tkernel=hyperparameters[\"svm\"][\"kernel\"],\n",
    "\t\tC=hyperparameters[\"svm\"][\"Œª\"],\n",
    "\t\tprobability=hyperparameters[\"svm\"][\"probability\"],\n",
    "\t\trandom_state=hyperparameters[\"svm\"][\"random_state\"]\n",
    "\t)\n",
    "\tsvm_clf.fit(X_train, y_train.ravel())\n",
    "\tmodels.append((\"svm\", svm_clf))\n",
    "\n",
    "\treturn models\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "def generate_predictions_stack(hyperparameters):\n",
    "\tstart_time = time.time()\n",
    "\tmodels = train_models(hyperparameters)\n",
    "\tstacking_clf = StackingClassifier(\n",
    "\t\testimators=models,\n",
    "\t\tfinal_estimator=LogisticRegression(),\n",
    "\t\tcv=5\n",
    "\t)\n",
    "\tstacking_clf.fit(X_train, y_train.ravel())\n",
    "\tpredictions = stacking_clf.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/stack/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/stack/\", \"predictions.csv\")\n",
    "\tsubmission = pd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t})\n",
    "\tsubmission.to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "hyperparameters = {\n",
    "\t\"l2amgdlogreg\": {\"Œ≤\": 128, \"œÑ\": 2000, \"Œ±\": 0.0825, \"Œª\": 0.01},\n",
    "\t\"knn\": {\"n_neighbors\": 5},\n",
    "\t\"dtree\": {\"random_state\": 42, \"ùíπ\": 10, \"ùìà\": 2, \"‚Ñì\": 1},\n",
    "\t\"rforest\": {\"random_state\": 42, \"Œ∑\": 100},\n",
    "\t\"svm\": {\"kernel\": \"linear\", \"Œª\": 1.0, \"probability\": True, \"random_state\": 42}\n",
    "}\n",
    "\n",
    "generate_predictions_stack(hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
