{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cogito\n",
    "αβκτνδςιηλε"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Definitions\n",
    "- Let `X` be a `m`-by-`n` matrix of input data, and `y` be a `m`-by-1 vector of output data, where `m` is the # of data points, and `n` is the # of features, so that `X` represents the features and `y` represents the labels.\n",
    "- Let `w` be a `n`-by-1 vector of weights, and `b` be a scalar of bias.\n",
    "- Let `dw` be the loss gradient w.r.t. `w`, and `db` be the loss gradient w.r.t. `b`, where for least squares we have ${dw=\\frac{1}{m}\\sum_{i=1}^{m}(ŷ^{(i)} - y^{(i)})x^{(i)}+\\frac{\\lambda w}{m}}$ and ${db = \\frac{1}{m}\\sum_{i=1}^{m}(ŷ^{(i)} - y^{(i)})}$.\n",
    "- Let `β` be the batch size.\n",
    "- Let `τ` be the # of epochs.\n",
    "- Let `α` be the learning rate.\n",
    "- Let `κ` be the # of components (where `n` is the maximum possible # of components).\n",
    "- Let `ν` be the variance ratio.\n",
    "- Let `δ` be the maximum depth.\n",
    "- Let `ς` be the minimum number of samples to split an internal node.\n",
    "- Let `ι` be the minimum number of leaves at a leaf node.\n",
    "- Let `η` be the number of estimators.\n",
    "- Let `λ` be the regularisation parameter.\n",
    "- Let `ε` be a small positive constant.\n",
    "\n",
    "### Naming Conventions\n",
    "1. Dataset is treated as a single word.\n",
    "2. Featureset refers to the features of a dataset.\n",
    "3. Labelset refers to the labels of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- numpy, pandas, and sklearn (scikit-learn) must be installed and available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for operating system functions.\n",
    "import os\n",
    "# Import numpy for mathematical computation.\n",
    "import numpy as np\n",
    "# Import pandas for data manipulation.\n",
    "import pandas as pd\n",
    "# Import datetime for local time retrieval.\n",
    "from datetime import datetime\n",
    "# Import logging for file logging.\n",
    "import logging\n",
    "# Import time for duration measurement.\n",
    "import time\n",
    "\n",
    "# Import PCA, StandardScaler, and KNeighborsClassifier for PCA-KNN model.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Import DecisionTreeClassifier for dtree model.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Import RandomForestClassifier for rforest model.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import KFold and f1_score for cross-validation.\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Initialise file logging configuration.\n",
    "logging.basicConfig(filename=\"main.log\", level=logging.INFO, format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Load train and test datasets.\n",
    "S_train = pd.read_csv(\"train.csv\")\n",
    "S_train_tfidf = pd.read_csv(\"train_tfidf_features.csv\")\n",
    "S_test = pd.read_csv(\"test.csv\")\n",
    "S_test_tfidf = pd.read_csv(\"test_tfidf_features.csv\")\n",
    "\n",
    "# Extract train features, train labels, and test features.\n",
    "X_train = S_train_tfidf.iloc[:, 2:].values\n",
    "y_train = S_train[\"label\"].values.reshape(-1, 1)\n",
    "X_test = S_test_tfidf.iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Mini-Batch Gradient Descent Logistic Regression Model (mgdlogreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return σ(z) for some z.\n",
    "def σ(z): return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Return cross-entropy loss for some y and ŷ.\n",
    "# def loss(y, ŷ):\n",
    "# \tm, _ = y.shape\n",
    "# \treturn -1/m * np.sum(y * np.log(ŷ) + (1 - y) * np.log(1 - ŷ))\n",
    "\n",
    "# Return dw and db, for some X, y, and ŷ.\n",
    "def gradients(X, y, ŷ):\n",
    "\tm, _ = X.shape\n",
    "\tdw = 1/m * np.dot(X.T, (ŷ - y))\n",
    "\tdb = 1/m * np.sum(ŷ - y)\n",
    "\treturn dw, db\n",
    "\n",
    "# Return w and b from mini-batch gradient descent on X and y, for batch size β, epochs τ, and learning rate α.\n",
    "def train_mgdlogreg(X, y, β, τ, α):\n",
    "\tm, n = X.shape\n",
    "\t# Initialise w to 0 vector and b to 0.\n",
    "\tw, b = np.zeros((n, 1)), 0\n",
    "\t# For each epoch:\n",
    "\tfor epoch in range(τ):\n",
    "\t\t# For every batch within epoch:\n",
    "\t\tfor i in range(0, m, β):\n",
    "\t\t\tX_batch, y_batch = X[i:i+β], y[i:i+β]\n",
    "\t\t\tŷ = σ(np.dot(X_batch, w) + b)\n",
    "\t\t\tdw, db = gradients(X_batch, y_batch, ŷ)\n",
    "\t\t\tw, b = w - α*dw, b - α*db\n",
    "\treturn w, b\n",
    "\n",
    "# Return array of predictions, where each prediction is 1 if corresponding ŷ entry > 0.5, and 0 otherwise.\n",
    "def predict_mgdlogreg(X, w, b):\n",
    "\t# Compute ŷ = σ(w⋅X + b).\n",
    "\tŷ = σ(np.dot(X, w) + b)\n",
    "\treturn [1 if p > 0.5 else 0 for p in ŷ]\n",
    "\n",
    "# Train model, make model predictions, and save model predictions to CSV file, for some β, τ, and α.\n",
    "def generate_predictions_mgdlogreg(X_train, y_train, X_test, β, τ, α):\n",
    "\tstart_time = time.time()\n",
    "\tw, b = train_mgdlogreg(X_train, y_train, β, τ, α)\n",
    "\tos.makedirs(\"./predictions/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/\", f\"mgdlogreg(β={β},τ={τ},α={α}).csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predict_mgdlogreg(X_test, w, b)\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# generate_predictions_mgdlogreg(X_train, y_train, X_test, 128, 1200, 0.084)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: PCA-KNN Model (pcaknn)\n",
    "### Usage Examples\n",
    "1. `generate_predictions_pcaknn(X_train, y_train, X_test, 0.95, \"ν\")`\n",
    "2. `generate_predictions_pcaknn(X_train, y_train, X_test, 2000, \"κ\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return PCA-transformed train and test featuresets, for some ν or κ.\n",
    "def apply_pca(X_train, X_test, mode_value, mode):\n",
    "\t# Validate mode and mode value.\n",
    "\tif mode not in [\"ν\", \"κ\"] or (mode == \"ν\" and not (0 < mode_value <= 1)) or (mode == \"κ\" and not (mode_value > 1 and isinstance(mode_value, int))): raise ValueError(\"Cogito: mode must either be ν with value between 0 and 1, or κ with mode_value as an integer greater than 1.\")\n",
    "\t# Instantiate PCA and StandardScaler objects.\n",
    "\tpca = PCA(n_components=mode_value)\n",
    "\tscaler = StandardScaler()\n",
    "\t# PCA-transform train and test featuresets.\n",
    "\tX_train_pca = pca.fit_transform(scaler.fit_transform(X_train))\n",
    "\tX_test_pca = pca.transform(scaler.transform(X_test))\n",
    "\t# Return PCA-transformed featuresets and # of components.\n",
    "\treturn X_train_pca, X_test_pca, pca.n_components_\n",
    "\n",
    "# Return KNN predictions for PCA-transformed train and test datasets.\n",
    "def train_and_predict_knn(X_train_pca, y_train, X_test_pca):\n",
    "\t# Instantiate KNeighborsClassifier object.\n",
    "\tknn = KNeighborsClassifier(n_neighbors=2)\n",
    "\tknn.fit(X_train_pca, y_train)\n",
    "\treturn knn.predict(X_test_pca)\n",
    "\n",
    "# Train model, make model predictions, and save model predictions to CSV file, for some ν or κ.\n",
    "def generate_predictions_pcaknn(X_train, y_train, X_test, mode_value, mode):\n",
    "\t# Validate mode and mode value.\n",
    "\tif mode not in [\"ν\", \"κ\"] or (mode == \"ν\" and not (0 < mode_value <= 1)) or (mode == \"κ\" and not (mode_value > 1 and isinstance(mode_value, int))): raise ValueError(\"Cogito: mode must either be ν with value between 0 and 1, or κ with mode_value as an integer greater than 1.\")\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tX_train_pca, X_test_pca, n_components = apply_pca(X_train, X_test, mode_value, mode)\n",
    "\tos.makedirs(\"./predictions/pcaknn/\", exist_ok=True)\n",
    "\tif mode == \"ν\": file_name = os.path.join(\"./predictions/\", f\"pcaknn(ν={mode_value},κ={n_components}).csv\")\n",
    "\telse: file_name = os.path.join(\"./predictions/\", f\"pcaknn(κ={n_components}).csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": train_and_predict_knn(X_train_pca, y_train, X_test_pca)\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# generate_predictions_pcaknn(X_train, y_train, X_test, 5000, \"κ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Other Models\n",
    "### Decision Tree Model (dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_dtree(X_train, y_train, X_test, S_test, δ=None, ς=2, ι=1):\n",
    "\tstart_time = time.time()\n",
    "\tmodel = DecisionTreeClassifier(max_depth=δ, min_samples_split=ς, min_samples_leaf=ι)\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\tpredictions = model.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/dtree/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/dtree/\", f\"δ={δ},ς={ς},ι={ι}.csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# train_and_predict_dtree(X_train, y_train, X_test, S_test, δ=50, ς=10, ι=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model (rforest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rforest(X_train, y_train, X_test, S_test, δ=None, ς=2, ι=1, η=100):\n",
    "\tstart_time = time.time()\n",
    "\trf = RandomForestClassifier(max_depth=δ, min_samples_split=ς, min_samples_leaf=ι, n_estimators=η)\n",
    "\trf.fit(X_train, y_train)\n",
    "\tpredictions = rf.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/rforest/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/rforest/\", f\"δ={δ},ς={ς},ι={ι},η={η}.csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# train_and_predict_rforest(X_train, y_train, X_test, S_test, δ=10000, ς=10, ι=2, η=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Adagrad Mini-Batch Gradient Descent Logistic Regression Model (l2amgdlogreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions file ./predictions/l2amgdlogreg/β=128,τ=1000,α=0.0825,λ=0.01.csv generated in 673.67s.\n"
     ]
    }
   ],
   "source": [
    "# Return σ(z) for some z.\n",
    "def σ(z): return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Return dw and db, for some X, y, ŷ, and λ.\n",
    "def gradients(X, y, ŷ, w, λ):\n",
    "\tm, _ = X.shape\n",
    "\tdw = 1/m * np.dot(X.T, (ŷ - y)) + (λ/m) * w\n",
    "\tdb = 1/m * np.sum(ŷ - y)\n",
    "\treturn dw, db\n",
    "\n",
    "# Return w and b from L2 adagrad mini-batch gradient descent on X and y, for batch size β, epochs τ, initial learning rate α, and regularisation λ.\n",
    "def train_l2amgdlogreg(β, τ, α, λ=0, ε=1e-8):\n",
    "\tm, n = X_train.shape\n",
    "\t# Initialise w to 0 vector and b to 0.\n",
    "\tw, b = np.zeros((n, 1)), 0\n",
    "\t# Initialise gradient accumulation variables for Adagrad.\n",
    "\tsdw, sdb = np.zeros((n, 1)), 0\n",
    "\t# For each epoch:\n",
    "\tfor epoch in range(τ):\n",
    "\t\t# For every batch within epoch:\n",
    "\t\tfor i in range(0, m, β):\n",
    "\t\t\tX_batch, y_batch = X_train[i:i+β], y_train[i:i+β]\n",
    "\t\t\tŷ = σ(np.dot(X_batch, w) + b)\n",
    "\t\t\tdw, db = gradients(X_batch, y_batch, ŷ, w, λ)\n",
    "\t\t\tsdw += dw ** 2\n",
    "\t\t\tsdb += db ** 2\n",
    "\t\t\tw -= (α / (np.sqrt(sdw) + ε)) * dw\n",
    "\t\t\tb -= (α / (np.sqrt(sdb) + ε)) * db\n",
    "\treturn w, b\n",
    "\n",
    "# Return array of predictions, where each prediction is 1 if corresponding ŷ entry > 0.5, and 0 otherwise.\n",
    "def predict_l2amgdlogreg(w, b):\n",
    "\t# Compute ŷ = σ(w⋅X + b).\n",
    "\tŷ = σ(np.dot(X_test, w) + b)\n",
    "\treturn [1 if p > 0.5 else 0 for p in ŷ]\n",
    "\n",
    "# Train model, make model predictions, and save model predictions to CSV file, for some β, τ, α, and λ.\n",
    "def generate_predictions_l2amgdlogreg(β, τ, α, λ=0):\n",
    "\tstart_time = time.time()\n",
    "\tw, b = train_l2amgdlogreg(β, τ, α, λ)\n",
    "\tos.makedirs(\"./predictions/l2amgdlogreg/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/l2amgdlogreg/\", f\"β={β},τ={τ},α={α},λ={λ}.csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predict_l2amgdlogreg(w, b)\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "generate_predictions_l2amgdlogreg(β=128, τ=1000, α=0.0825, λ=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
