{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for operating system functions.\n",
    "import os\n",
    "# Import numpy for mathematical computation.\n",
    "import numpy as np\n",
    "# Import pandas for data manipulation.\n",
    "import pandas as pd\n",
    "# Import datetime for local time retrieval.\n",
    "from datetime import datetime\n",
    "# Import logging for file logging.\n",
    "import logging\n",
    "# Import time for duration measurement.\n",
    "import time\n",
    "\n",
    "# Import random, clone, make_scorer, and _fit_and_score for simulated annealing.\n",
    "import random\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection._validation import _fit_and_score\n",
    "\n",
    "# Import StandardScaler for data pre-processing.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Import train_test_split for model selection.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import KFold and f1_score for cross-validation.\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Import SK, XGB, and CatBoost classifiers.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "\tfilename=\"main.log\",\n",
    "\tlevel=logging.INFO,\n",
    "\tformat=\"%(asctime)s - %(message)s\",\n",
    "\tdatefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "PARAMETER_MAPPINGS = {\n",
    "\t\"α\": \"learning_rate\",\n",
    "\t\"τ\": \"max_iter\",\n",
    "\t\"θ\": \"max_leaf_nodes\",\n",
    "\t\"Δ\": \"max_depth\",\n",
    "\t\"l\": \"min_samples_leaf\",\n",
    "\t\"seed\": \"random_state\"\n",
    "}\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "\t\"SKLknn\": KNeighborsClassifier,\n",
    "\t\"SKLsvm\": SVC,\n",
    "\t\"SKLrf\": RandomForestClassifier,\n",
    "\t\"SKLgb\": GradientBoostingClassifier,\n",
    "\t\"SKLhgb\": HistGradientBoostingClassifier,\n",
    "\t\"XGBgb\": XGBClassifier,\n",
    "\t\"CBgb\": CatBoostClassifier\n",
    "}\n",
    "\n",
    "# Load train and test datasets.\n",
    "S_train = pd.read_csv(\"./data/train.csv\")\n",
    "S_train_tfidf = pd.read_csv(\"./data/train_tfidf_features.csv\")\n",
    "S_test = pd.read_csv(\"./data/test.csv\")\n",
    "S_test_tfidf = pd.read_csv(\"./data/test_tfidf_features.csv\")\n",
    "\n",
    "# Extract train features, train labels, and test features.\n",
    "X_train = S_train_tfidf.iloc[:, 2:].values\n",
    "y_train = S_train[\"label\"].values.reshape(-1, 1)\n",
    "X_test = S_test_tfidf.iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logreg Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def σ(z): return 1 / (1 + np.exp(-z))\n",
    "def bce_loss(y, ŷ): return (-1/(y.shape[0])) * np.sum(y * np.log(ŷ) + (1 - y) * np.log(1 - ŷ))\n",
    "\n",
    "# Return dw and db, for some X, y, ŷ, w, R, and λ.\n",
    "def gradients_logreg(X, y, ŷ, w, R=None, λ=0):\n",
    "\tm, _ = X.shape\n",
    "\tdw = 1/m * np.dot(X.T, (ŷ - y))\n",
    "\tdb = 1/m * np.sum(ŷ - y)\n",
    "\tif R == \"L2\":\n",
    "\t\tdw += λ * w / m\n",
    "\telif R == \"L1\":\n",
    "\t\tdw += λ * np.sign(w) / m\n",
    "\treturn dw, db\n",
    "\n",
    "# Return (w, b) from gradient descent on X_train and y_train, for some τ, α, G, β, R, and λ.\n",
    "def train_logreg(X_train, y_train, τ=1000, α=0.1, G=\"mini-batch\", β=128, R=None, λ=0):\n",
    "\tm, n = X_train.shape\n",
    "\tw, b = np.zeros((n, 1)), 0\n",
    "\tfor epoch in range(τ):\n",
    "\t\tif G == \"full-batch\":\n",
    "\t\t\tX_batch, y_batch = X_train, y_train\n",
    "\t\t\tŷ = σ(np.dot(X_batch, w) + b)\n",
    "\t\t\tdw, db = gradients_logreg(X_batch, y_batch, ŷ, w, R, λ)\n",
    "\t\t\tw, b = w - α*dw, b - α*db\n",
    "\t\telif G == \"mini-batch\":\n",
    "\t\t\tfor i in range(0, m, β):\n",
    "\t\t\t\tX_batch, y_batch = X_train[i:i+β], y_train[i:i+β]\n",
    "\t\t\t\tŷ = σ(np.dot(X_batch, w) + b)\n",
    "\t\t\t\tdw, db = gradients_logreg(X_batch, y_batch, ŷ, w, R, λ)\n",
    "\t\t\t\tw, b = w - α*dw, b - α*db\n",
    "\t\telif G == \"stochastic\":\n",
    "\t\t\tfor i in range(m):\n",
    "\t\t\t\tX_batch, y_batch = X_train[i:i+1], y_train[i:i+1]\n",
    "\t\t\t\tŷ = σ(np.dot(X_batch, w) + b)\n",
    "\t\t\t\tdw, db = gradients_logreg(X_batch, y_batch, ŷ, w, R, λ)\n",
    "\t\t\t\tw, b = w - α*dw, b - α*db\n",
    "\treturn w, b\n",
    "\n",
    "# Return array of predictions, where each prediction is 1 if corresponding ŷ entry > 0.5, and 0 otherwise.\n",
    "def predict_logreg(wb_tuple, X):\n",
    "\tw, b = wb_tuple\n",
    "\tŷ = σ(np.dot(X, w) + b)\n",
    "\treturn np.array([1 if p > 0.5 else 0 for p in ŷ])\n",
    "\n",
    "# Train model, make predictions, and save predictions to CSV file.\n",
    "def generate_predictions_logreg(τ=1000, α=0.1, G=\"mini-batch\", β=128, R=None, λ=0):\n",
    "\tstart_time = time.time()\n",
    "\tw, b = train_logreg(np.array(X_train), np.array(y_train), τ, α, G, β, R, λ)\n",
    "\tpredictions = predict_logreg((w, b), np.array(X_test))\n",
    "\tos.makedirs(\"./predictions/logreg/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/logreg/\", f\"τ={τ},α={α},G={G},β={β},R={R},λ={λ}.csv\")\n",
    "\tpd.DataFrame({\"id\": S_test[\"id\"], \"label\": predictions}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(x):\n",
    "\tscaler = StandardScaler()\n",
    "\tX_train_scaled = scaler.fit_transform(X_train)\n",
    "\tX_test_scaled = scaler.transform(X_test)\n",
    "\tif 0 <= x <= 1:\n",
    "\t\t# x represents variance threshold.\n",
    "\t\tpca = PCA(n_components=None)\n",
    "\t\tpca.fit(X_train_scaled)\n",
    "\t\tc = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= x) + 1\n",
    "\t\tv = x\n",
    "\telse:\n",
    "\t\t# x represents number of components.\n",
    "\t\tpca = PCA(n_components=x)\n",
    "\t\tpca.fit(X_train_scaled)\n",
    "\t\tc = x\n",
    "\t\tv = sum(pca.explained_variance_ratio_)\n",
    "\t# Transform train and test datasets.\n",
    "\tX_train_pca = pca.transform(X_train_scaled)\n",
    "\tX_test_pca = pca.transform(X_test_scaled)\n",
    "\treturn X_train_pca, X_test_pca, c, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLknn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SKLknn(X_train, y_train, k=5, W=\"uniform\", p=2, m=\"minkowski\"):\n",
    "\tmodel = KNeighborsClassifier(\n",
    "\t\tn_neighbors=k,\n",
    "\t\tweights=W,\n",
    "\t\tp=p,\n",
    "\t\tmetric=m,\n",
    "\t\tn_jobs=-1\n",
    "\t)\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\treturn model\n",
    "\n",
    "def predict_SKLknn(model, X): return model.predict(X)\n",
    "\n",
    "def generate_predictions_SKLknn(k=5, W=\"uniform\", p=2, m=\"minkowski\"):\n",
    "\tstart_time = time.time()\n",
    "\tmodel = train_SKLknn(np.array(X_train), np.array(y_train), k, W, p, m)\n",
    "\tpredictions = predict_SKLknn(model, np.array(X_test))\n",
    "\tos.makedirs(\"./predictions/SKLknn/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/SKLknn/\", f\"k={k},W={W},p={p},m={m}.csv\")\n",
    "\tpd.DataFrame({\"id\": S_test[\"id\"], \"label\": predictions}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model, make model predictions, and save model predictions to CSV file.\n",
    "def generate_predictions_pcaknn(x):\n",
    "\tstart_time = time.time()\n",
    "\tX_train_pca, X_test_pca, c, v = apply_pca(x)\n",
    "\tmodel = train_SKLknn(X_train_pca, y_train, k=2)\n",
    "\tpredictions = predict_SKLknn(model, X_test_pca)\n",
    "\tos.makedirs(\"./predictions/pcaknn/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/pcaknn/\", f\"pcaknn(ρ={c},ν={v:.2f}).csv\")\n",
    "\tpd.DataFrame({\"id\": S_test[\"id\"], \"label\": predictions}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLlogreg Model (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLrf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SKLrf(X_train, y_train, η=100, C=\"gini\", Δ=None, ψ=2, l=1, θ=None, seed=None):\n",
    "\tmodel = RandomForestClassifier(\n",
    "\t\tn_estimators=η,\n",
    "\t\tcriterion=C,\n",
    "\t\tmax_depth=Δ,\n",
    "\t\tmin_samples_split=ψ,\n",
    "\t\tmin_samples_leaf=l,\n",
    "\t\tmax_leaf_nodes=θ,\n",
    "\t\tn_jobs=-1,\n",
    "\t\trandom_state=seed\n",
    "\t)\n",
    "\tmodel.fit(X_train, y_train.ravel())\n",
    "\treturn model\n",
    "\n",
    "def predict_SKLrf(model, X): return model.predict(X)\n",
    "\n",
    "def generate_predictions_SKLrf(η=100, C=\"gini\", Δ=None, ψ=2, l=1, θ=None, seed=None):\n",
    "\tstart_time = time.time()\n",
    "\tmodel = train_SKLrf(np.array(X_train), np.array(y_train), η, C, Δ, ψ, l, θ, seed)\n",
    "\tpredictions = predict_SKLrf(model, np.array(X_test))\n",
    "\tos.makedirs(\"./predictions/SKLrf/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/SKLrf/\", f\"η={η},C={C},Δ={Δ},ψ={ψ},l={l},θ={θ},seed={seed}.csv\")\n",
    "\tpd.DataFrame({\"id\": S_test[\"id\"], \"label\": predictions}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLsvm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model, make predictions, and save predictions to CSV file.\n",
    "def train_SKLsvm(X_train, y_train, λ=1.0, K=\"rbf\", d=3, γ=\"scale\", τ=-1, seed=None):\n",
    "\tmodel = SVC(\n",
    "\t\tC=λ,\n",
    "\t\tkernel=K,\n",
    "\t\tdegree=d,\n",
    "\t\tgamma=γ,\n",
    "\t\tmax_iter=τ,\n",
    "\t\trandom_state=seed\n",
    "\t)\n",
    "\tmodel.fit(X_train, y_train.ravel())\n",
    "\treturn model\n",
    "\n",
    "def predict_SKLsvm(model, X): return model.predict(X)\n",
    "\n",
    "def generate_predictions_SKLsvm(λ=1.0, K=\"rbf\", d=3, γ=\"scale\", τ=-1, seed=None):\n",
    "\tstart_time = time.time()\n",
    "\tmodel = train_SKLsvm(np.array(X_train), np.array(y_train), λ, K, d, γ, τ, seed)\n",
    "\tpredictions = predict_SKLsvm(model, np.array(X_test))\n",
    "\tos.makedirs(\"./predictions/SKLsvm/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/SKLsvm/\", f\"λ={λ},K={K},d={d},γ={γ},τ={τ},seed={seed}.csv\")\n",
    "\tpd.DataFrame({\"id\": S_test[\"id\"], \"label\": predictions}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLgb Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SKLgb(X_train, y_train, α=0.1, η=100, ss=1.0, ψ=2, l=1, Δ=3, seed=None, θ=None):\n",
    "\tmodel = GradientBoostingClassifier(\n",
    "\t\tlearning_rate=α,\n",
    "\t\tn_estimators=η,\n",
    "\t\tsubsample=ss,\n",
    "\t\tmin_samples_split=ψ,\n",
    "\t\tmin_samples_leaf=l,\n",
    "\t\tmax_depth=Δ,\n",
    "\t\trandom_state=seed,\n",
    "\t\tmax_leaf_nodes=θ\n",
    "\t)\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\treturn model\n",
    "\n",
    "def predict_SKLgb(model, X): return model.predict(X)\n",
    "\n",
    "def generate_predictions_SKLgb(α=0.1, η=100, ss=1.0, ψ=2, l=1, Δ=3, seed=None, θ=None):\n",
    "\tstart_time = time.time()\n",
    "\tmodel = train_SKLgb(np.array(X_train), np.array(y_train), α, η, ss, ψ, l, Δ, seed, θ)\n",
    "\tpredictions = predict_SKLgb(model, np.array(X_test))\n",
    "\tos.makedirs(\"./predictions/SKLgb/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/SKLgb/\", f\"α={α},η={η},ss={ss},ψ={ψ},l={l},Δ={Δ},seed={seed},θ={θ}.csv\")\n",
    "\tpd.DataFrame({\"id\": S_test[\"id\"], \"label\": predictions}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLhgb Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SKLhgb(X_train, y_train, α=0.1, τ=100, θ=31, Δ=None, l=20, seed=None):\n",
    "\tmodel = HistGradientBoostingClassifier(\n",
    "\t\tlearning_rate=α,\n",
    "\t\tmax_iter=τ,\n",
    "\t\tmax_leaf_nodes=θ,\n",
    "\t\tmax_depth=Δ,\n",
    "\t\tmin_samples_leaf=l,\n",
    "\t\trandom_state=seed\n",
    "\t)\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\treturn model\n",
    "\n",
    "def predict_SKLhgb(model, X): return model.predict(X)\n",
    "\n",
    "def generate_predictions_SKLhgb(α=0.1, τ=100, θ=31, Δ=None, l=20, seed=None):\n",
    "\tstart_time = time.time()\n",
    "\tmodel = train_SKLhgb(np.array(X_train), np.array(y_train), α, τ, θ, Δ, l, seed)\n",
    "\tpredictions = predict_SKLhgb(model, np.array(X_test))\n",
    "\tos.makedirs(\"./predictions/SKLhgb/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/SKLhgb/\", f\"α={α},τ={τ},θ={θ},Δ={Δ},l={l},seed={seed}.csv\")\n",
    "\tsubmission = pd.DataFrame({\"id\": S_test[\"id\"], \"label\": predictions}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBgb Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_XGBgb(X_train, y_train, η=100, Δ=3, θ=0, α=0.1, O=\"binary:logistic\", B=\"gbtree\", γ=0, ss=1, λ=0, Λ=1, seed=None):\n",
    "\txgb_model = XGBClassifier(\n",
    "\t\tn_estimators=η,\n",
    "\t\tmax_depth=Δ,\n",
    "\t\tmax_leaves=θ if θ > 0 else None,\n",
    "\t\tlearning_rate=α,\n",
    "\t\tobjective=O,\n",
    "\t\tbooster=B,\n",
    "\t\tgamma=γ,\n",
    "\t\tsubsample=ss,\n",
    "\t\treg_alpha=λ,\n",
    "\t\treg_lambda=Λ,\n",
    "\t\trandom_state=seed,\n",
    "\t\tn_jobs=-1\n",
    "\t)\n",
    "\txgb_model.fit(X_train, y_train.ravel())\n",
    "\treturn xgb_model\n",
    "\n",
    "def predict_XGBgb(model, X):\n",
    "\treturn model.predict(X)\n",
    "\n",
    "def generate_predictions_XGBgb(η=100, Δ=3, θ=0, α=0.1, O=\"binary:logistic\", B=\"gbtree\", γ=0, ss=1, λ=0, Λ=1, seed=None):\n",
    "\tstart_time = time.time()\n",
    "\txgb_model = train_XGBgb(np.array(X_train), np.array(y_train), η, Δ, θ, α, O, B, γ, ss, λ, Λ, seed)\n",
    "\tpredictions = predict_XGBgb(xgb_model, np.array(X_test))\n",
    "\tos.makedirs(\"./predictions/XGBgb/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/XGBgb/\", f\"η={η},Δ={Δ},θ={θ},α={α},O={O},B={B},γ={γ},ss={ss},λ={λ},Λ={Λ},seed={seed}.csv\")\n",
    "\tpd.DataFrame({\"id\": S_test[\"id\"], \"label\": predictions}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBgb Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CBgb(X_train, y_train, α=0.1, τ=100, Δ=6, l=3, seed=None):\n",
    "\tmodel = CatBoostClassifier(\n",
    "\t\tlearning_rate=α,\n",
    "\t\titerations=τ,\n",
    "\t\tdepth=Δ,\n",
    "\t\tl2_leaf_reg=l,\n",
    "\t\trandom_seed=seed\n",
    "\t)\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\treturn model\n",
    "\n",
    "def predict_CBgb(model, X): return model.predict(X)\n",
    "\n",
    "def generate_predictions_CBgb(α=0.1, τ=1, Δ=6, l=3, seed=None):\n",
    "\tstart_time = time.time()\n",
    "\tmodel = train_CBgb(np.array(X_train), np.array(y_train), α, τ, Δ, l, seed)\n",
    "\tpredictions = predict_CBgb(model, np.array(X_test))\n",
    "\tos.makedirs(\"./predictions/CBgb/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/CBgb/\", f\"α={α},τ={τ},Δ={Δ},l={l},seed={seed}.csv\")\n",
    "\tpd.DataFrame({\"id\": S_test[\"id\"], \"label\": predictions}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidate(train_fn, predict_fn, grid, X, y, k=2):\n",
    "\tkf = KFold(n_splits=k, shuffle=True)\n",
    "\tbest_f1 = -np.inf\n",
    "\tbest_hyperparameters = None\n",
    "\n",
    "\tfor hyperparameters in grid:\n",
    "\t\tf1_scores = []\n",
    "\t\tfor i_train, i_val in kf.split(X):\n",
    "\t\t\tX_train, X_val = X[i_train], X[i_val]\n",
    "\t\t\ty_train, y_val = y[i_train], y[i_val]\n",
    "\t\t\tmodel = train_fn(X_train, y_train, **hyperparameters)\n",
    "\t\t\ty_pred = predict_fn(model, X_val)\n",
    "\t\t\tf1_scores.append(f1_score(y_val, y_pred, average=\"macro\"))\n",
    "\t\tmean_f1 = np.mean(f1_scores)\n",
    "\t\tif mean_f1 > best_f1:\n",
    "\t\t\tbest_f1 = mean_f1\n",
    "\t\t\tbest_hyperparameters = hyperparameters\n",
    "\tprint(f\"Best hyperparameters in grid:\", best_hyperparameters)\n",
    "\tprint(f\"Best {k}-fold cross-validation f1 score:\", best_f1)\n",
    "\treturn best_hyperparameters, best_f1\n",
    "\n",
    "def crossvalidate_and_generate_predictions(model_name, grid, k=5):\n",
    "\ttrain_fn = globals().get(f\"train_{model_name.lower()}\")\n",
    "\tpredict_fn = globals().get(f\"predict_{model_name.lower()}\")\n",
    "\tgenerate_predictions_fn = globals().get(f\"generate_predictions_{model_name.lower()}\")\n",
    "\tif not train_fn or not predict_fn or not generate_predictions_fn:\n",
    "\t\traise ValueError(f\"Expected train_{model_name} or predict_{model_name} to exist.\")\n",
    "\tbest_hyperparameters, _ = crossvalidate(train_fn, predict_fn, grid, X_train, y_train, k=k)\n",
    "\tgenerate_predictions_fn(**best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = [\n",
    "# \t{\"τ\": 10000, \"α\": 0.0825, \"G\": \"mini-batch\", \"β\": 128, \"R\": \"L2\", \"λ\": 1},\n",
    "# \t{\"τ\": 10000, \"α\": 0.085, \"G\": \"mini-batch\", \"β\": 128, \"R\": \"L2\", \"λ\": 1}\n",
    "# ]\n",
    "# crossvalidate_and_generate_predictions(\"logreg\", grid, k=2)\n",
    "\n",
    "# grid = [\n",
    "# \t{\"η\": 100, \"C\": \"gini\", \"Δ\": None, \"ψ\": 2, \"l\": 1, \"θ\": None, \"seed\": None},\n",
    "# \t{\"η\": 200, \"C\": \"entropy\", \"Δ\": 10, \"ψ\": 3, \"l\": 2, \"θ\": 50, \"seed\": 42}\n",
    "# ]\n",
    "# crossvalidate_and_generate_predictions(\"SKLrf\", grid, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Annealing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accept_prob(old_score, new_score, T):\n",
    "\tT += 0.01\n",
    "\treturn np.exp((new_score - old_score) / T)\n",
    "\n",
    "def dt(t0, t1): return t1 - t0 if t0 is not None else 0\n",
    "\n",
    "class CustomSimulatedAnnealing:\n",
    "\tdef __init__(self, estimator, param_grid, scoring=f1_score, T=5, T_min=0.1, alpha=0.9, n_trans=5, max_iter=1, max_runtime=60, cv=2, verbose=True, refit=False, n_jobs=-1, max_score=np.inf):\n",
    "\t\tself.estimator = estimator\n",
    "\t\tself.param_grid = param_grid\n",
    "\t\tself.scoring = make_scorer(scoring)\n",
    "\t\tself.T = T\n",
    "\t\tself.T_min = T_min\n",
    "\t\tself.alpha = alpha\n",
    "\t\tself.n_trans = n_trans\n",
    "\t\tself.max_iter = max_iter\n",
    "\t\tself.max_runtime = max_runtime\n",
    "\t\tself.cv = cv\n",
    "\t\tself.verbose = verbose\n",
    "\t\tself.refit = refit\n",
    "\t\tself.n_jobs = n_jobs\n",
    "\t\tself.max_score = max_score\n",
    "\n",
    "\t\tself.best_params_ = None\n",
    "\t\tself.best_score_ = None\n",
    "\t\tself.best_estimator_ = None\n",
    "\t\tself.grid_scores_ = None\n",
    "\t\tself.runtime_ = None\n",
    "\t\tself._set_dynamic_params()\n",
    "\n",
    "\tdef _set_dynamic_params(self):\n",
    "\t\tnum_params = np.prod([len(v) for v in self.param_grid.values()])\n",
    "\t\tself.T = 10 * num_params\n",
    "\t\tself.T_min = 0.01 * self.T\n",
    "\t\tself.alpha = 0.9\n",
    "\n",
    "\tdef fit(self, X, y):\n",
    "\t\tif isinstance(X, pd.DataFrame):\n",
    "\t\t\tX = X.to_numpy()\n",
    "\t\tif isinstance(y, pd.DataFrame):\n",
    "\t\t\ty = y.to_numpy()\n",
    "\t\telif isinstance(y, (list, pd.Series)):\n",
    "\t\t\ty = np.array(y)\n",
    "\n",
    "\t\tT = self.T\n",
    "\t\tT_min = self.T_min\n",
    "\t\talpha = self.alpha\n",
    "\t\tmax_iter = self.max_iter\n",
    "\t\tn_trans = self.n_trans\n",
    "\t\tparam_grid = self.param_grid\n",
    "\t\tmax_runtime = self.max_runtime\n",
    "\t\tcv = self.cv\n",
    "\n",
    "\t\told_params = {k: np.random.choice(v) for k, v in param_grid.items()}\n",
    "\t\told_est = clone(self.estimator)\n",
    "\t\told_est.set_params(**old_params)\n",
    "\t\told_score, old_std = self._evaluate_score(old_est, X, y, cv)\n",
    "\n",
    "\t\tbest_score = old_score\n",
    "\t\tbest_params = old_params\n",
    "\t\tstates_checked = {tuple(sorted(old_params.items())): (old_score, old_std)}\n",
    "\t\ttotal_iter = 1\n",
    "\t\tgrid_scores = [(1, T, old_score, old_std, old_params)]\n",
    "\n",
    "\t\ttime_at_start = time.time()\n",
    "\t\tt_elapsed = dt(time_at_start, time.time())\n",
    "\n",
    "\t\twhile T > T_min and total_iter < max_iter and t_elapsed < max_runtime and best_score < self.max_score:\n",
    "\t\t\tfor _ in range(self.n_trans):\n",
    "\t\t\t\tnew_params = self._generate_new_params(old_params, param_grid)\n",
    "\t\t\t\tnew_score, new_std = self._evaluate_score_for_params(new_params, X, y, cv, states_checked)\n",
    "\n",
    "\t\t\t\tif new_score >= self.max_score:\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\tgrid_scores.append((total_iter, T, new_score, new_std, new_params))\n",
    "\n",
    "\t\t\t\tif new_score > best_score:\n",
    "\t\t\t\t\tbest_score = new_score\n",
    "\t\t\t\t\tbest_params = new_params\n",
    "\n",
    "\t\t\t\tif self.verbose:\n",
    "\t\t\t\t\tprint(f\"{total_iter} T: {T:.5f}, score: {new_score:.6f}, std: {new_std:.6f}, params: {new_params}\")\n",
    "\n",
    "\t\t\t\tif accept_prob(old_score, new_score, T) > random.random():\n",
    "\t\t\t\t\told_params = new_params\n",
    "\t\t\t\t\told_score = new_score\n",
    "\n",
    "\t\t\t\tt_elapsed = dt(time_at_start, time.time())\n",
    "\t\t\t\ttotal_iter += 1\n",
    "\n",
    "\t\t\tif new_score >= self.max_score:\n",
    "\t\t\t\tprint(f\"Max score reached {new_score}!\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tT *= alpha\n",
    "\n",
    "\t\tif self.refit:\n",
    "\t\t\tself.estimator.set_params(**best_params)\n",
    "\t\t\tself.estimator.fit(X, y)\n",
    "\t\t\tself.best_estimator_ = self.estimator\n",
    "\n",
    "\t\tself.runtime_ = t_elapsed\n",
    "\t\tself.grid_scores_ = grid_scores\n",
    "\t\tself.best_score_ = best_score\n",
    "\t\tself.best_params_ = best_params\n",
    "\n",
    "\tdef _generate_new_params(self, old_params, param_grid):\n",
    "\t\tnew_params = old_params.copy()\n",
    "\t\trand_key = np.random.choice(list(param_grid.keys()))\n",
    "\t\tval = param_grid[rand_key]\n",
    "\t\tif hasattr(val, 'rvs'):\n",
    "\t\t\tnew_params[rand_key] = val.rvs()\n",
    "\t\telse:\n",
    "\t\t\tsample_space = [v for v in val if v != old_params[rand_key]]\n",
    "\t\t\tnew_params[rand_key] = np.random.choice(sample_space) if sample_space else np.random.choice(val)\n",
    "\t\treturn new_params\n",
    "\n",
    "\tdef _evaluate_score_for_params(self, params, X, y, cv, states_checked):\n",
    "\t\tparams_tuple = tuple(sorted(params.items()))\n",
    "\t\tif params_tuple in states_checked:\n",
    "\t\t\treturn states_checked[params_tuple]\n",
    "\t\telse:\n",
    "\t\t\test = clone(self.estimator)\n",
    "\t\t\test.set_params(**params)\n",
    "\t\t\tscore, std = self._evaluate_score(est, X, y, cv)\n",
    "\t\t\tstates_checked[params_tuple] = (score, std)\n",
    "\t\t\treturn score, std\n",
    "\n",
    "\tdef _evaluate_score(self, estimator, X, y, cv):\n",
    "\t\tif self.n_jobs > 1:\n",
    "\t\t\tout = Parallel(n_jobs=self.n_jobs)(\n",
    "\t\t\t\tdelayed(_fit_and_score)(clone(estimator), X, y, self.scoring, train, test, self.verbose,\n",
    "\t\t\t\t\t\t\t\t\t\t{}, {}, return_parameters=False, error_score='raise')\n",
    "\t\t\t\tfor train, test in KFold(cv).split(X)\n",
    "\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tscores = []\n",
    "\t\t\tfor train, test in KFold(cv).split(X):\n",
    "\t\t\t\testimator.fit(X[train], y[train])\n",
    "\t\t\t\tscores.append(self.scoring(estimator, X[test], y[test]))\n",
    "\t\t\tout = (np.mean(scores), np.std(scores))\n",
    "\t\treturn out\n",
    "\n",
    "def simulated_annealing(model_name, param_grid, T=10, T_min=0.001, alpha=0.9, n_trans=5, max_iter=100, max_runtime=60, cv=5):\n",
    "\tparam_grid_mapped = {PARAMETER_MAPPINGS.get(k, k): v for k, v in param_grid.items()}\n",
    "\tmodel_class = MODEL_CLASSES.get(model_name)\n",
    "\tif model_class is None: raise ValueError(f\"Model '{model_name}' is not recognized.\")\n",
    "\tmodel = model_class()\n",
    "\tsa = CustomSimulatedAnnealing(\n",
    "\t\testimator=model,\n",
    "\t\tparam_grid=param_grid_mapped,\n",
    "\t\tscoring=f1_score,\n",
    "\t\tT=T,\n",
    "\t\tT_min=T_min,\n",
    "\t\talpha=alpha,\n",
    "\t\tn_trans=n_trans,\n",
    "\t\tmax_iter=max_iter,\n",
    "\t\tmax_runtime=max_runtime,\n",
    "\t\tcv=cv\n",
    "\t)\n",
    "\tsa.fit(X_train, y_train)\n",
    "\treturn sa.best_estimator_, sa.best_score_, sa.best_params_, sa.grid_scores_, sa.runtime_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 T: 20.00000, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 T: 20.00000, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "3 T: 20.00000, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "4 T: 20.00000, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "5 T: 20.00000, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "6 T: 18.00000, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "7 T: 18.00000, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "8 T: 18.00000, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "9 T: 18.00000, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "10 T: 18.00000, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "11 T: 16.20000, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "12 T: 16.20000, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "13 T: 16.20000, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "14 T: 16.20000, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "15 T: 16.20000, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "16 T: 14.58000, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "17 T: 14.58000, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "18 T: 14.58000, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "19 T: 14.58000, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "20 T: 14.58000, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "21 T: 13.12200, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "22 T: 13.12200, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "23 T: 13.12200, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "24 T: 13.12200, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "25 T: 13.12200, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "26 T: 11.80980, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "27 T: 11.80980, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "28 T: 11.80980, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "29 T: 11.80980, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "30 T: 11.80980, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "31 T: 10.62882, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "32 T: 10.62882, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "33 T: 10.62882, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "34 T: 10.62882, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "35 T: 10.62882, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "36 T: 9.56594, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "37 T: 9.56594, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "38 T: 9.56594, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "39 T: 9.56594, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "40 T: 9.56594, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "41 T: 8.60934, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "42 T: 8.60934, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "43 T: 8.60934, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "44 T: 8.60934, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "45 T: 8.60934, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "46 T: 7.74841, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "47 T: 7.74841, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "48 T: 7.74841, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "49 T: 7.74841, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "50 T: 7.74841, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "51 T: 6.97357, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "52 T: 6.97357, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "53 T: 6.97357, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "54 T: 6.97357, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "55 T: 6.97357, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "56 T: 6.27621, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "57 T: 6.27621, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "58 T: 6.27621, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "59 T: 6.27621, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "60 T: 6.27621, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "61 T: 5.64859, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "62 T: 5.64859, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "63 T: 5.64859, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "64 T: 5.64859, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "65 T: 5.64859, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "66 T: 5.08373, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "67 T: 5.08373, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "68 T: 5.08373, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "69 T: 5.08373, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "70 T: 5.08373, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "71 T: 4.57536, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "72 T: 4.57536, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "73 T: 4.57536, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "74 T: 4.57536, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "75 T: 4.57536, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "76 T: 4.11782, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "77 T: 4.11782, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "78 T: 4.11782, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "79 T: 4.11782, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "80 T: 4.11782, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "81 T: 3.70604, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "82 T: 3.70604, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "83 T: 3.70604, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "84 T: 3.70604, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "85 T: 3.70604, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "86 T: 3.33544, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "87 T: 3.33544, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "88 T: 3.33544, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "89 T: 3.33544, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "90 T: 3.33544, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "91 T: 3.00189, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "92 T: 3.00189, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "93 T: 3.00189, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "94 T: 3.00189, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n",
      "95 T: 3.00189, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "96 T: 2.70170, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "97 T: 2.70170, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "98 T: 2.70170, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "99 T: 2.70170, score: 0.526579, std: 0.010613, params: {'learning_rate': 0.08, 'max_iter': 50}\n",
      "100 T: 2.70170, score: 0.539027, std: 0.010463, params: {'learning_rate': 0.1, 'max_iter': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "\t\"α\": [0.08, 0.1],\n",
    "\t\"τ\": [50],\n",
    "\t# \"θ\": [31, 50],\n",
    "\t# \"Δ\": [None, 10],\n",
    "\t# \"l\": [20]\n",
    "}\n",
    "\n",
    "best_model, best_score, best_params, grid_scores, runtime = simulated_annealing(\n",
    "\tmodel_name=\"SKLhgb\",\n",
    "\tparam_grid=param_grid\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
