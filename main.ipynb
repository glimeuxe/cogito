{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cogito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "- `m` is # of data points.\n",
    "- `n` is # of features.\n",
    "- `X` is `m`-by-`n` matrix representing features.\n",
    "- `y` is `m`-by-1 vector representing labels.\n",
    "- `w` is `n`-by-1 vector representing weights.\n",
    "- `b` is scalar representing bias.\n",
    "- `dw` is loss gradient w.r.t. `w`.\n",
    "- `db` is loss gradient w.r.t. `b`.\n",
    "\n",
    "### logreg(œÑ,Œ±,G,Œ≤,R,Œª)\n",
    "1. `œÑ` is # of epochs: non-negative integer.\n",
    "2. `Œ±` is learning rate: float in range ${[0,1]}$.\n",
    "3. `G` is gradient descent type: `\"full-batch\"`, `\"mini-batch\"`, xor `\"stochastic\"`.\n",
    "4. `Œ≤` is batch size: non-negative integer xor `None`.\n",
    "5. `R` is regularisation type: `\"L2\"`, `\"L1\"`, xor `None`.\n",
    "6. `Œª` is regularisation degree: real in range ${[0,\\infin)}$.\n",
    "\n",
    "### pcaknn(œÅ,ŒΩ)\n",
    "1. `œÅ` is # of principal components: integer in range ${[0,n]}$.\n",
    "2. `ŒΩ` is variance ratio: float in range ${[0,1]}$.\n",
    "\n",
    "### svm(Œª,K,p,Œ≥,Œ∫,œÑ,seed)\n",
    "1. `Œª` is margin regularisation degree.\n",
    "2. `K` is kernel type: `\"linear\"`, `\"poly\"`, `\"rbf\"`, `\"sigmoid\"`, xor `\"precomputed\"`.\n",
    "3. `p` is polynomial kernel degree for polynomial kernel type: non-negative integer.\n",
    "4. `Œ≥` is kernel coefficient for RBF, polynomial, xor sigmoid kernel type: `\"scale\"`, `\"auto\"`, xor float in range ${[0,1]}$.\n",
    "5. `Œ∫` is independent term of kernel function with effect for polynomial xor sigmoid kernel type.\n",
    "6. `œÑ` is maximum # of iterations: non-negative integer xor `-1`.\n",
    "7. `seed` is pseudorandom random state: integer xor `None`.\n",
    "\n",
    "#### rforest(Œ∑,C,Œî,œà,‚Ñì,Œ∏,Œ¥,B,seed)\n",
    "1. `Œ∑` is # of estimators: non-negative integer.\n",
    "2. `C` is criterion to measure split quality: `\"entropy\"`, `\"log_loss\"`, xor `\"gini\"`.\n",
    "3. `Œî` is maximum tree depth: non-negative integer xor `None`.\n",
    "4. `œà` is minimum # of samples needed to split internal node: non-negative integer.\n",
    "5. `‚Ñì` is minimum # of samples needed at leaf node: non-negative integer.\n",
    "6. `Œ∏` is maximum # of leaf nodes: non-negative integer xor `None`.\n",
    "7. `Œ¥` is minimum impurity decrease: float in range ${[0,1]}$.\n",
    "8. `B` is whether bootstrapping is used: `False` xor `True`.\n",
    "9. `seed` is pseudorandom random state: integer xor `None`.\n",
    "\n",
    "### gbdtree(Œ±,Œ∑,ss,œà,‚Ñì,Œî,Œ¥,seed,Œ∏)\n",
    "1. `Œ±` is learning rate: float in range ${[0,1]}$.\n",
    "2. `Œ∑` is # of estimators: non-negative integer.\n",
    "3. `ss` is fraction of samples used to fit individual base learners: float in range ${[0,1]}$.\n",
    "4. `œà` is minimum # of samples needed to split internal node: non-negative integer.\n",
    "5. `‚Ñì` is minimum # of samples needed at leaf node: non-negative integer.\n",
    "6. `Œî` is maximum tree depth: non-negative integer xor `None`.\n",
    "7. `Œ¥` is minimum impurity decrease: float in range ${[0,1]}$.\n",
    "8. `seed` is pseudorandom random state: integer xor `None`.\n",
    "9. `Œ∏` is maximum # of leaf nodes: non-negative integer xor `None`.\n",
    "\n",
    "#### hgbdtree(Œ±,œÑ,Œ∏,Œî,l,seed)\n",
    "1. `Œ±` is learning rate: float in range ${[0,1]}$.\n",
    "2. `œÑ` is maximum # of iterations: non-negative integer.\n",
    "3. `Œ∏` is maximum # of leaf nodes: non-negative integer xor `None`.\n",
    "4. `Œî` is maximum tree depth: non-negative integer xor `None`.\n",
    "5. `l` is minimum # of samples needed at leaf node: non-negative integer xor `None`.\n",
    "6. `seed` is pseudorandom random state: integer xor `None`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- numpy, pandas, and sklearn (scikit-learn) must be installed and available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for operating system functions.\n",
    "import os\n",
    "# Import numpy for mathematical computation.\n",
    "import numpy as np\n",
    "# Import pandas for data manipulation.\n",
    "import pandas as pd\n",
    "# Import datetime for local time retrieval.\n",
    "from datetime import datetime\n",
    "# Import logging for file logging.\n",
    "import logging\n",
    "# Import time for duration measurement.\n",
    "import time\n",
    "\n",
    "# Import KFold and f1_score for cross-validation.\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Import PCA and KNeighborsClassifier for pcaknn.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Import RandomForestClassifier for rforest.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Import SVC for svm.\n",
    "from sklearn.svm import SVC\n",
    "# Import GradientBoostingClassifier for gbdtree.\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Import HistGradientBoostingClassifier for hgbdtree.\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# Import xgboost for xgbdtree.\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialise file logging configuration.\n",
    "logging.basicConfig(\n",
    "\tfilename=\"main.log\",\n",
    "\tlevel=logging.INFO,\n",
    "\tformat=\"%(asctime)s - %(message)s\",\n",
    "\tdatefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Load train and test datasets.\n",
    "S_train = pd.read_csv(\"./data/train.csv\")\n",
    "S_train_tfidf = pd.read_csv(\"./data/train_tfidf_features.csv\")\n",
    "S_test = pd.read_csv(\"./data/test.csv\")\n",
    "S_test_tfidf = pd.read_csv(\"./data/test_tfidf_features.csv\")\n",
    "\n",
    "# Extract train features, train labels, and test features.\n",
    "X_train = S_train_tfidf.iloc[:, 2:].values\n",
    "y_train = S_train[\"label\"].values.reshape(-1, 1)\n",
    "X_test = S_test_tfidf.iloc[:, 1:].values\n",
    "\n",
    "# Return œÉ(z) for some z.\n",
    "def œÉ(z): return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Return cross-entropy loss for some y and yÃÇ.\n",
    "def loss(y, yÃÇ): return (-1/(y.shape[0])) * np.sum(y * np.log(yÃÇ) + (1 - y) * np.log(1 - yÃÇ))\n",
    "\n",
    "# Return PCA-transformed train and test featuresets, for either some ŒΩ or œÅ.\n",
    "def apply_pca(mode_value, mode):\n",
    "\t# Instantiate PCA and StandardScaler objects.\n",
    "\tpca = PCA(n_components=mode_value)\n",
    "\tscaler = StandardScaler()\n",
    "\t# PCA-transform train and test featuresets.\n",
    "\tX_train_pca = pca.fit_transform(scaler.fit_transform(X_train))\n",
    "\tX_test_pca = pca.transform(scaler.transform(X_test))\n",
    "\t# Return PCA-transformed featuresets and # of components.\n",
    "\treturn X_train_pca, X_test_pca, pca.n_components_\n",
    "\n",
    "def crossvalidate(train_fn, predict_fn, grid, X, y, k=2):\n",
    "\tkf = KFold(n_splits=k, shuffle=True, random_state=7)\n",
    "\tbest_score = -np.inf\n",
    "\tbest_hyperparameters = None\n",
    "\tfor hyperparameters in grid:\n",
    "\t\tscores = []\n",
    "\t\tfor train_index, val_index in kf.split(X):\n",
    "\t\t\tX_train, X_val = X[train_index], X[val_index]\n",
    "\t\t\ty_train, y_val = y[train_index], y[val_index]\n",
    "\t\t\tprint(hyperparameters)\n",
    "\t\t\tmodel = train_fn(X_train, y_train, **hyperparameters)\n",
    "\t\t\ty_pred = predict_fn(model, X_val)\n",
    "\t\t\tscores.append(f1_score(y_val, y_pred, average='macro'))\n",
    "\t\tmean_score = np.mean(scores)\n",
    "\t\tif mean_score > best_score:\n",
    "\t\t\tbest_score = mean_score\n",
    "\t\t\tbest_hyperparameters = hyperparameters\n",
    "\treturn best_hyperparameters, best_score\n",
    "\n",
    "def crossvalidate_and_generate_predictions(model_name, grid, k=5):\n",
    "\ttrain_fn = globals().get(f\"train_{model_name.lower()}\")\n",
    "\tpredict_fn = globals().get(f\"predict_{model_name.lower()}\")\n",
    "\tgenerate_predictions_fn = globals().get(f\"generate_predictions_{model_name.lower()}\")\n",
    "\n",
    "\tif not train_fn or not predict_fn or not generate_predictions_fn:\n",
    "\t\traise ValueError(f\"Functions for '{model_name}' not found. Ensure that 'train_{model_name.lower()}', 'predict_{model_name.lower()}', and 'generate_predictions_{model_name.lower()}' exist.\")\n",
    "\n",
    "\tbest_hyperparameters, best_score = crossvalidate(train_fn, predict_fn, grid, X_train, y_train, k=k)\n",
    "\n",
    "\tprint(f\"Best hyperparameters for {model_name}:\", best_hyperparameters)\n",
    "\tprint(f\"Best cross-validation f1 score for {model_name}:\", best_score)\n",
    "\n",
    "\tgenerate_predictions_fn(**best_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "### Logistic Regression Model (`logreg`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for logreg: {'œÑ': 1, 'Œ±': 0.0825, 'G': 'mini-batch', 'Œ≤': 128, 'R': None, 'Œª': 0}\n",
      "Best cross-validation f1 score for logreg: 0.38224160599504164\n",
      "Predictions file ./predictions/logreg/œÑ=1,Œ±=0.0825,G=mini-batch,Œ≤=128,R=None,Œª=0.csv generated in 1.00s.\n"
     ]
    }
   ],
   "source": [
    "# Return dw and db, for some X, y, yÃÇ, w, Œª, and R.\n",
    "def gradients_logreg(X, y, yÃÇ, w, R=None, Œª=0):\n",
    "\tm, _ = X.shape\n",
    "\tdw = 1/m * np.dot(X.T, (yÃÇ - y))\n",
    "\tdb = 1/m * np.sum(yÃÇ - y)\n",
    "\tif R == \"L2\":\n",
    "\t\tdw += Œª * w / m\n",
    "\telif R == \"L1\":\n",
    "\t\tdw += Œª * np.sign(w) / m\n",
    "\treturn dw, db\n",
    "\n",
    "# Return w and b from gradient descent on X and y, for some œÑ, Œ±, G, Œ≤, R, and Œª.\n",
    "def train_logreg(X_train, y_train, œÑ=1000, Œ±=0.1, G=\"mini-batch\", Œ≤=128, R=None, Œª=0):\n",
    "\tm, n = X_train.shape\n",
    "\tw, b = np.zeros((n, 1)), 0\n",
    "\tfor epoch in range(œÑ):\n",
    "\t\tif G == \"full-batch\":\n",
    "\t\t\tX_batch, y_batch = X_train, y_train\n",
    "\t\t\tyÃÇ = œÉ(np.dot(X_batch, w) + b)\n",
    "\t\t\tdw, db = gradients_logreg(X_batch, y_batch, yÃÇ, w, R, Œª)\n",
    "\t\t\tw, b = w - Œ±*dw, b - Œ±*db\n",
    "\t\telif G == \"mini-batch\":\n",
    "\t\t\tfor i in range(0, m, Œ≤):\n",
    "\t\t\t\tX_batch, y_batch = X_train[i:i+Œ≤], y_train[i:i+Œ≤]\n",
    "\t\t\t\tyÃÇ = œÉ(np.dot(X_batch, w) + b)\n",
    "\t\t\t\tdw, db = gradients_logreg(X_batch, y_batch, yÃÇ, w, R, Œª)\n",
    "\t\t\t\tw, b = w - Œ±*dw, b - Œ±*db\n",
    "\t\telif G == \"stochastic\":\n",
    "\t\t\tfor i in range(m):\n",
    "\t\t\t\tX_batch, y_batch = X_train[i:i+1], y_train[i:i+1]\n",
    "\t\t\t\tyÃÇ = œÉ(np.dot(X_batch, w) + b)\n",
    "\t\t\t\tdw, db = gradients_logreg(X_batch, y_batch, yÃÇ, w, R, Œª)\n",
    "\t\t\t\tw, b = w - Œ±*dw, b - Œ±*db\n",
    "\treturn w, b\n",
    "\n",
    "# Return array of predictions, where each prediction is 1 if corresponding yÃÇ entry > 0.5, and 0 otherwise.\n",
    "def predict_logreg(wb_tuple, X):\n",
    "\tw, b = wb_tuple\n",
    "\tyÃÇ = œÉ(np.dot(X, w) + b)\n",
    "\treturn np.array([1 if p > 0.5 else 0 for p in yÃÇ])\n",
    "\n",
    "# Train model, make model predictions, and save model predictions to CSV file, for some Œ≤, œÑ, Œ±, Œª, L, and G.\n",
    "def generate_predictions_logreg(œÑ=1000, Œ±=0.1, G=\"mini-batch\", Œ≤=128, R=None, Œª=0):\n",
    "\tstart_time = time.time()\n",
    "\tw, b = train_logreg(np.array(X_train), np.array(y_train), œÑ, Œ±, G, Œ≤, R, Œª)\n",
    "\tpredictions = predict_logreg((w, b), np.array(X_test))\n",
    "\tos.makedirs(\"./predictions/logreg/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/logreg/\", f\"œÑ={œÑ},Œ±={Œ±},G={G},Œ≤={Œ≤},R={R},Œª={Œª}.csv\")\n",
    "\tsubmission = pd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t})\n",
    "\tsubmission.to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "crossvalidate_and_generate_predictions(\n",
    "\tmodel_name=\"logreg\",\n",
    "\tgrid=[\n",
    "\t\t{\"œÑ\": 1, \"Œ±\": 0.0825, \"G\": \"mini-batch\", \"Œ≤\": 128, \"R\": None, \"Œª\": 0},\n",
    "\t\t{\"œÑ\": 2, \"Œ±\": 0.085, \"G\": \"full-batch\", \"Œ≤\": 64, \"R\": \"L2\", \"Œª\": 0.1},\n",
    "\t],\n",
    "\tk=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_logreg(œÑ=1000, Œ±=0.1, G=\"mini-batch\", Œ≤=128, R=None, Œª=0):\n",
    "# \tm, n = X_train.shape\n",
    "# \tw, b = np.zeros((n, 1)), 0\n",
    "# \tfor epoch in range(œÑ):\n",
    "# \t\tif G == \"full-batch\":\n",
    "# \t\t\tX_batch, y_batch = X_train, y_train\n",
    "# \t\t\tyÃÇ = œÉ(np.dot(X_batch, w) + b)\n",
    "# \t\t\tdw, db = gradients_logreg(X_batch, y_batch, yÃÇ, w, R, Œª)\n",
    "# \t\t\tw, b = w - Œ±*dw, b - Œ±*db\n",
    "# \t\telif G == \"mini-batch\":\n",
    "# \t\t\tfor i in range(0, m, Œ≤):\n",
    "# \t\t\t\tX_batch, y_batch = X_train[i:i+Œ≤], y_train[i:i+Œ≤]\n",
    "# \t\t\t\tyÃÇ = œÉ(np.dot(X_batch, w) + b)\n",
    "# \t\t\t\tdw, db = gradients_logreg(X_batch, y_batch, yÃÇ, w, R, Œª)\n",
    "# \t\t\t\tw, b = w - Œ±*dw, b - Œ±*db\n",
    "# \t\telif G == \"stochastic\":\n",
    "# \t\t\tfor i in range(m):\n",
    "# \t\t\t\tX_batch, y_batch = X_train[i:i+1], y_train[i:i+1]\n",
    "# \t\t\t\tyÃÇ = œÉ(np.dot(X_batch, w) + b)\n",
    "# \t\t\t\tdw, db = gradients_logreg(X_batch, y_batch, yÃÇ, w, R, Œª)\n",
    "# \t\t\t\tw, b = w - Œ±*dw, b - Œ±*db\n",
    "# \treturn w, b\n",
    "\n",
    "# def predict_logreg(w, b):\n",
    "# \t# Compute yÃÇ = œÉ(w ‚ãÖ X + b).\n",
    "# \tyÃÇ = œÉ(np.dot(X_test, w) + b)\n",
    "# \treturn [1 if p > 0.5 else 0 for p in yÃÇ]\n",
    "\n",
    "# def generate_predictions_logreg(œÑ=1000, Œ±=0.1, G=\"mini-batch\", Œ≤=128, R=None, Œª=0):\n",
    "# \tstart_time = time.time()\n",
    "# \tw, b = train_logreg(œÑ, Œ±, G, Œ≤, R, Œª)\n",
    "# \tos.makedirs(\"./predictions/logreg/\", exist_ok=True)\n",
    "# \tfile_name = os.path.join(\"./predictions/logreg/\", f\"œÑ={œÑ},Œ±={Œ±},G={G},Œ≤={Œ≤},R={R},Œª={Œª}.csv\")\n",
    "# \tpd.DataFrame({\n",
    "# \t\t\"id\": S_test[\"id\"],\n",
    "# \t\t\"label\": predict_logreg(w, b)\n",
    "# \t}).to_csv(file_name, index=False)\n",
    "# \tend_time = time.time()\n",
    "# \t# Log and print success message.\n",
    "# \tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "# \tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "### PCA-KNN Model (`pcaknn`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model, make model predictions, and save model predictions to CSV file, for some ŒΩ or œÅ.\n",
    "def generate_predictions_pcaknn(mode_value, mode=\"œÅ\"):\n",
    "\t# Validate mode and mode value.\n",
    "\tif mode not in [\"ŒΩ\", \"œÅ\"] or (mode == \"ŒΩ\" and not (0 < mode_value <= 1)) or (mode == \"œÅ\" and not (mode_value > 1 and isinstance(mode_value, int))): raise ValueError(\"Mode must either be ŒΩ with value between 0 and 1, or œÅ with mode_value as an integer greater than 1.\")\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tX_train_pca, X_test_pca, n_components = apply_pca(mode_value, mode)\n",
    "\tknn = KNeighborsClassifier(n_neighbors=2)\n",
    "\tknn.fit(X_train_pca, y_train)\n",
    "\tpredictions = knn.predict(X_test_pca)\n",
    "\tos.makedirs(\"./predictions/pcaknn/\", exist_ok=True)\n",
    "\tif mode == \"ŒΩ\":\n",
    "\t\tfile_name = os.path.join(\"./predictions/pcaknn/\", f\"pcaknn(ŒΩ={mode_value},œÅ={n_components}).csv\")\n",
    "\telse:\n",
    "\t\tfile_name = os.path.join(\"./predictions/pcaknn/\", f\"pcaknn(œÅ={n_components}).csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# generate_predictions_pcaknn(mode_value=0.95, mode=\"ŒΩ\")\n",
    "# generate_predictions_pcaknn(mode_value=1000, mode=\"œÅ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model (`rforest`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RandomForest classifier, make predictions, and save predictions to CSV file.\n",
    "def generate_predictions_rforest(Œ∑=100, C=\"gini\", Œî=None, œà=2, ‚Ñì=1, Œ∏=None, Œ¥=0.0, B=True, seed=None):\n",
    "\tstart_time = time.time()\n",
    "\trf = RandomForestClassifier(\n",
    "\t\tn_estimators=Œ∑,\n",
    "\t\tcriterion=C,\n",
    "\t\tmax_depth=Œî,\n",
    "\t\tmin_samples_split=œà,\n",
    "\t\tmin_samples_leaf=‚Ñì,\n",
    "\t\tmax_leaf_nodes=Œ∏,\n",
    "\t\tmin_impurity_decrease=Œ¥,\n",
    "\t\tbootstrap=B,\n",
    "\t\tn_jobs=-1,\n",
    "\t\trandom_state=seed\n",
    "\t)\n",
    "\trf.fit(X_train, y_train.ravel())\n",
    "\tpredictions = rf.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/rforest/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/rforest/\", f\"Œ∑={Œ∑},C={C},Œî={Œî},œà={œà},‚Ñì={‚Ñì},Œ∏={Œ∏},Œ¥={Œ¥},B={B},seed={seed}.csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# generate_predictions_rforest(Œ∑=100, C=\"gini\", Œî=None, œà=2, ‚Ñì=1, Œ∏=None, Œ¥=0.0, B=True, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model (`svm`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM classifier, make predictions, and save predictions to CSV file.\n",
    "def generate_predictions_svm(Œª=1.0, K=\"rbf\", p=3, Œ≥=\"scale\", Œ∫=0.0, œÑ=-1, seed=None):\n",
    "\tstart_time = time.time()\n",
    "\tsvm_clf = SVC(\n",
    "\t\tC=Œª,\n",
    "\t\tkernel=K,\n",
    "\t\tdegree=p,\n",
    "\t\tgamma=Œ≥,\n",
    "\t\tcoef0=Œ∫,\n",
    "\t\tmax_iter=œÑ,\n",
    "\t\trandom_state=seed\n",
    "\t)\n",
    "\tsvm_clf.fit(X_train, y_train.ravel())\n",
    "\tpredictions = svm_clf.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/svm/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/svm/\", f\"Œª={Œª},K={K},p={p},Œ≥={Œ≥},Œ∫={Œ∫},œÑ={œÑ},seed={seed}.csv\")\n",
    "\tsubmission = pd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t})\n",
    "\tsubmission.to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "# generate_predictions_svm(Œª=1.0, K=\"rbf\", p=3, Œ≥=\"scale\", Œ∫=0.0, œÑ=-1, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Decision Tree Model (`gbdtree`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_gbdtree(Œ±=0.1, Œ∑=100, ss=1.0, œà=2, ‚Ñì=1, Œî=3, Œ¥=0.0, seed=None, Œ∏=None):\n",
    "\tstart_time = time.time()\n",
    "\tgbc = GradientBoostingClassifier(\n",
    "\t\tlearning_rate=Œ±,\n",
    "\t\tn_estimators=Œ∑,\n",
    "\t\tsubsample=ss,\n",
    "\t\tmin_samples_split=œà,\n",
    "\t\tmin_samples_leaf=‚Ñì,\n",
    "\t\tmax_depth=Œî,\n",
    "\t\tmin_impurity_decrease=Œ¥,\n",
    "\t\trandom_state=seed,\n",
    "\t\tmax_leaf_nodes=Œ∏\n",
    "\t)\n",
    "\tgbc.fit(X_train, y_train)\n",
    "\tpredictions = gbc.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/gbdtree/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/gbdtree/\", f\"Œ±={Œ±},Œ∑={Œ∑},ss={ss},œà={œà},‚Ñì={‚Ñì},Œî={Œî},Œ¥={Œ¥},seed={seed},Œ∏={Œ∏}.csv\")\n",
    "\tsubmission = pd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t})\n",
    "\tsubmission.to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram Gradient Boosted Decision Tree Model (`hgbdtree`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Œ±': 0.1, 'œÑ': 1, 'Œ∏': 31, 'Œî': None, 'l': 20, 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Œ±': 0.1, 'œÑ': 1, 'Œ∏': 31, 'Œî': None, 'l': 20, 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Œ±': 0.1, 'œÑ': 1, 'Œ∏': 31, 'Œî': None, 'l': 20, 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Œ±': 0.1, 'œÑ': 1, 'Œ∏': 31, 'Œî': None, 'l': 20, 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Œ±': 0.1, 'œÑ': 1, 'Œ∏': 31, 'Œî': None, 'l': 20, 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Œ±': 0.1, 'œÑ': 2, 'Œ∏': 31, 'Œî': None, 'l': 20, 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Œ±': 0.1, 'œÑ': 2, 'Œ∏': 31, 'Œî': None, 'l': 20, 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Œ±': 0.1, 'œÑ': 2, 'Œ∏': 31, 'Œî': None, 'l': 20, 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Œ±': 0.1, 'œÑ': 2, 'Œ∏': 31, 'Œî': None, 'l': 20, 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Œ±': 0.1, 'œÑ': 2, 'Œ∏': 31, 'Œî': None, 'l': 20, 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for hgbdtree: {'Œ±': 0.1, 'œÑ': 2, 'Œ∏': 31, 'Œî': None, 'l': 20, 'seed': 42}\n",
      "Best cross-validation f1 score for hgbdtree: 0.3840559402758433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions file ./predictions/hgbdtree/Œ±=0.1,œÑ=2,Œ∏=31,Œî=None,l=20,seed=42.csv generated in 3.86s.\n"
     ]
    }
   ],
   "source": [
    "def train_hgbdtree(X_train, y_train, Œ±=0.1, œÑ=100, Œ∏=31, Œî=None, l=20, seed=None):\n",
    "\thgbc = HistGradientBoostingClassifier(\n",
    "\t\tlearning_rate=Œ±,\n",
    "\t\tmax_iter=œÑ,\n",
    "\t\tmax_leaf_nodes=Œ∏,\n",
    "\t\tmax_depth=Œî,\n",
    "\t\tmin_samples_leaf=l,\n",
    "\t\trandom_state=seed\n",
    "\t)\n",
    "\thgbc.fit(X_train, y_train)\n",
    "\treturn hgbc\n",
    "\n",
    "def predict_hgbdtree(model, X):\n",
    "\treturn model.predict(X)\n",
    "\n",
    "def generate_predictions_hgbdtree(Œ±=0.1, œÑ=100, Œ∏=31, Œî=None, l=20, seed=None):\n",
    "\tstart_time = time.time()\n",
    "\thgbc = train_hgbdtree(np.array(X_train), np.array(y_train), Œ±, œÑ, Œ∏, Œî, l, seed)\n",
    "\tpredictions = predict_hgbdtree(hgbc, np.array(X_test))\n",
    "\tos.makedirs(\"./predictions/hgbdtree/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/hgbdtree/\", f\"Œ±={Œ±},œÑ={œÑ},Œ∏={Œ∏},Œî={Œî},l={l},seed={seed}.csv\")\n",
    "\tsubmission = pd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t})\n",
    "\tsubmission.to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "crossvalidate_and_generate_predictions(\n",
    "\tmodel_name=\"hgbdtree\",\n",
    "\tgrid=[\n",
    "\t\t{\"Œ±\": 0.1, \"œÑ\": 1, \"Œ∏\": 31, \"Œî\": None, \"l\": 20, \"seed\": 42},\n",
    "\t\t{\"Œ±\": 0.1, \"œÑ\": 2, \"Œ∏\": 31, \"Œî\": None, \"l\": 20, \"seed\": 42}\n",
    "\t],\n",
    "\tk=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours Model (`knn`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_knn(k=5, W=\"uniform\", algo=\"auto\", ‚Ñì=30, p=2, ùìÇ=\"minkowski\"):\n",
    "\tstart_time = time.time()\n",
    "\tknn = KNeighborsClassifier(\n",
    "\t\tn_neighbors=k,\n",
    "\t\tweights=W,\n",
    "\t\talgorithm=algo,\n",
    "\t\tleaf_size=‚Ñì,\n",
    "\t\tp=p,\n",
    "\t\tmetric=ùìÇ,\n",
    "\t\tn_jobs=-1\n",
    "\t)\n",
    "\tknn.fit(X_train, y_train)\n",
    "\tpredictions = knn.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/knn/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/knn/\", f\"k={k},W={W},algo={algo},‚Ñì={‚Ñì},p={p},ùìÇ={ùìÇ}.csv\")\n",
    "\tsubmission = pd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t})\n",
    "\tsubmission.to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model (`xgboost`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_xgbdtree(Œ∑=100, Œî=3, Œ∏=0, G=\"depthwise\", Œ±=0.1, O=\"binary:logistic\", B=\"gbtree\", Œ≥=0, ss=1, Œª=0, Œõ=1, seed=None):\n",
    "\tstart_time = time.time()\n",
    "\txgb_model = xgb.XGBClassifier(\n",
    "\t\tn_estimators=Œ∑,\n",
    "\t\tmax_depth=Œî,\n",
    "\t\tmax_leaves=Œ∏ if Œ∏ > 0 else None,\n",
    "\t\tgrow_policy=G,\n",
    "\t\tlearning_rate=Œ±,\n",
    "\t\tobjective=O,\n",
    "\t\tbooster=B,\n",
    "\t\tgamma=Œ≥,\n",
    "\t\tsubsample=ss,\n",
    "\t\treg_alpha=Œª,\n",
    "\t\treg_lambda=Œõ,\n",
    "\t\trandom_state=seed,\n",
    "\t\tn_jobs=-1\n",
    "\t)\n",
    "\txgb_model.fit(X_train, y_train.ravel())\n",
    "\tpredictions = xgb_model.predict(X_test)\n",
    "\tos.makedirs(\"./predictions/xgboost/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/xgboost/\", f\"Œ∑={Œ∑},Œî={Œî},Œ∏={Œ∏},G={G},Œ±={Œ±},O={O},B={B},Œ≥={Œ≥},ss={ss},Œª={Œª},Œõ={Œõ},seed={seed}.csv\")\n",
    "\tpd.DataFrame({\n",
    "\t\t\"id\": S_test[\"id\"],\n",
    "\t\t\"label\": predictions\n",
    "\t}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\t# Log and print success message.\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_voted_predictions(models_params, scores):\n",
    "\tprediction_files = []\n",
    "\tfor model, params in models_params:\n",
    "\t\tif model == \"hgbdtree\":\n",
    "\t\t\tfile_name = os.path.join(\"./predictions/hgbdtree/\", f\"Œ±={params[\"Œ±\"]},œÑ={params[\"œÑ\"]},Œ∏={params[\"Œ∏\"]},Œî={params[\"Œî\"]},‚Ñì={params[\"‚Ñì\"]},seed={params[\"seed\"]}.csv\")\n",
    "\t\t\tif not os.path.exists(file_name):\n",
    "\t\t\t\tfile_name = generate_predictions_hgbdtree(**params)\n",
    "\t\telif model == \"logreg\":\n",
    "\t\t\tfile_name = os.path.join(\"./predictions/logreg/\", f\"œÑ={params[\"œÑ\"]},Œ±={params[\"Œ±\"]},G={params[\"G\"]},Œ≤={params[\"Œ≤\"]},R={params[\"R\"]},Œª={params[\"Œª\"]}.csv\")\n",
    "\t\t\tif not os.path.exists(file_name):\n",
    "\t\t\t\tfile_name = generate_predictions_logreg(**params)\n",
    "\t\tprediction_files.append(file_name)\n",
    "\tpredictions_list = []\n",
    "\tids = None\n",
    "\tfor file in prediction_files:\n",
    "\t\tdf = pd.read_csv(file)\n",
    "\t\tif ids is None: ids = df[\"id\"].values\n",
    "\t\tpredictions_list.append(df[\"label\"].values)\n",
    "\n",
    "\tif not all(len(pred) == len(ids) for pred in predictions_list): raise ValueError(\"Mismatch in number of predictions across files\")\n",
    "\n",
    "\tpredictions_array = np.array(predictions_list)\n",
    "\tprobabilities = np.array(scores) / np.sum(scores)\n",
    "\tfinal_predictions = []\n",
    "\tfor i in range(len(ids)):\n",
    "\t\tmodel_predictions = predictions_array[:, i]\n",
    "\t\tweighted_votes = np.zeros(2)\n",
    "\t\tfor j in range(len(predictions_list)):\n",
    "\t\t\tweighted_votes[int(model_predictions[j])] += probabilities[j]\n",
    "\t\tfinal_label = np.argmax(weighted_votes)\n",
    "\t\tfinal_predictions.append(final_label)\n",
    "\tos.makedirs(\"./predictions/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/\", \"probabilistic_voting.csv\")\n",
    "\tsubmission = pd.DataFrame({\n",
    "\t\t\"id\": ids,\n",
    "\t\t\"label\": final_predictions\n",
    "\t})\n",
    "\tsubmission.to_csv(file_name, index=False)\n",
    "\tprint(f\"Probabilistic voting predictions saved to {file_name}\")\n",
    "\n",
    "models_params = [\n",
    "\t(\"logreg\", {\"œÑ\": 1000, \"Œ±\": 0.085, \"G\": \"mini-batch\", \"Œ≤\": 128, \"R\": None, \"Œª\": 0}),\n",
    "\t(\"hgbdtree\", {\"Œ±\": 0.1, \"œÑ\": 100, \"Œ∏\": None, \"Œî\": None, \"‚Ñì\": 20, \"seed\": 7}),\n",
    "\t(\"hgbdtree\", {\"Œ±\": 0.1, \"œÑ\": 1000, \"Œ∏\": 31, \"Œî\": None, \"‚Ñì\": 20, \"seed\": 8})\n",
    "]\n",
    "scores = [0.70475, 0.71533, 0.71628]\n",
    "\n",
    "generate_voted_predictions(models_params, scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
