{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "50.007 MACHINE LEARNING<br />\n",
    "2024 SUMMER<br />\n",
    "<b>COGITO</b><br />\n",
    "</p>\n",
    "\n",
    "https://sutd-1007485.notion.site/50-007-Project-Report\n",
    "https://docs.google.com/spreadsheets/d/1xIcica8zDbkq8prRumnc9A9_lcsVXxXDGon1YXelMTQ/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base import *\n",
    "from grid_search import cv_grid_search\n",
    "from random_search import cv_random_search\n",
    "from simulated_annealing import SimulatedAnnealing, cv_simulated_annealing\n",
    "\n",
    "logging.basicConfig(filename=\"log\", level=logging.INFO, format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def σ(z): return 1 / (1 + np.exp(-z))\n",
    "# def ce_loss(y, ŷ): return (-1/(y.shape[0])) * np.sum(y * np.log(ŷ) + (1 - y) * np.log(1 - ŷ))\n",
    "\n",
    "# Return dw and db, for some X, y, ŷ, w, R, and λ.\n",
    "def gradients_logreg(X, y, ŷ, w, R=None, λ=0):\n",
    "\tm, _ = X.shape\n",
    "\tdw = 1/m * np.dot(X.T, (ŷ - y))\n",
    "\tdb = 1/m * np.sum(ŷ - y)\n",
    "\tif R == \"L2\":\n",
    "\t\tdw += λ * w / m\n",
    "\telif R == \"L1\":\n",
    "\t\tdw += λ * np.sign(w) / m\n",
    "\treturn dw, db\n",
    "\n",
    "# Return (w, b) from gradient descent on X_train and y_train, for some τ, α, G, β, R, and λ.\n",
    "def train_logreg(X_train, y_train, τ=1000, α=0.1, G=\"mini-batch\", β=128, R=None, λ=0):\n",
    "\tm, n = X_train.shape\n",
    "\tw, b = np.zeros((n, 1)), 0\n",
    "\tfor epoch in range(τ):\n",
    "\t\tif G == \"full-batch\":\n",
    "\t\t\tX_batch, y_batch = X_train, y_train\n",
    "\t\t\tŷ = σ(np.dot(X_batch, w) + b)\n",
    "\t\t\tdw, db = gradients_logreg(X_batch, y_batch, ŷ, w, R, λ)\n",
    "\t\t\tw, b = w - α*dw, b - α*db\n",
    "\t\telif G == \"mini-batch\":\n",
    "\t\t\tfor i in range(0, m, β):\n",
    "\t\t\t\tX_batch, y_batch = X_train[i:i+β], y_train[i:i+β]\n",
    "\t\t\t\tŷ = σ(np.dot(X_batch, w) + b)\n",
    "\t\t\t\tdw, db = gradients_logreg(X_batch, y_batch, ŷ, w, R, λ)\n",
    "\t\t\t\tw, b = w - α*dw, b - α*db\n",
    "\t\telif G == \"stochastic\":\n",
    "\t\t\tfor i in range(m):\n",
    "\t\t\t\tX_batch, y_batch = X_train[i:i+1], y_train[i:i+1]\n",
    "\t\t\t\tŷ = σ(np.dot(X_batch, w) + b)\n",
    "\t\t\t\tdw, db = gradients_logreg(X_batch, y_batch, ŷ, w, R, λ)\n",
    "\t\t\t\tw, b = w - α*dw, b - α*db\n",
    "\treturn w, b\n",
    "\n",
    "# Return array of predictions, where each prediction is 1 if corresponding ŷ entry > 0.5, and 0 otherwise.\n",
    "def predict_logreg(wb_tuple, X):\n",
    "\tw, b = wb_tuple\n",
    "\tŷ = σ(np.dot(X, w) + b)\n",
    "\treturn np.array([1 if p > 0.5 else 0 for p in ŷ])\n",
    "\n",
    "# Train model, make predictions, and save predictions to CSV file.\n",
    "def generate_predictions_logreg(τ=1000, α=0.1, G=\"mini-batch\", β=128, R=None, λ=0):\n",
    "\tstart_time = time.time()\n",
    "\tw, b = train_logreg(np.array(X_train), np.array(y_train), τ, α, G, β, R, λ)\n",
    "\tpredictions = predict_logreg((w, b), np.array(X_test))\n",
    "\toutput_dir = \"./predictions/logreg/\"\n",
    "\tos.makedirs(output_dir, exist_ok=True)\n",
    "\tfile_name = os.path.join(output_dir, f\"predictions.csv\")\n",
    "\tpd.DataFrame({\"id\": S_test[\"id\"], \"label\": predictions}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions_logreg(τ=1, α=0.08, G=\"mini-batch\", β=128, R=None, λ=0)\n",
    "# generate_predictions_logreg(τ=1000, α=0.08, G=\"stochastic\", R=L2, λ=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "- F1 score for `c=100`: 0.55415.\n",
    "- F1 score for `c=500`: 0.54648.\n",
    "- F1 score for `c=1000`: 0.55766.\n",
    "- F1 score for `c=2000`: 0.54434."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(x):\n",
    "\tscaler = StandardScaler()\n",
    "\tX_train_scaled = scaler.fit_transform(X_train)\n",
    "\tX_test_scaled = scaler.transform(X_test)\n",
    "\tif 0 <= x <= 1:\n",
    "\t\t# x is variance threshold.\n",
    "\t\tpca = PCA(n_components=None)\n",
    "\t\tpca.fit(X_train_scaled)\n",
    "\t\tc = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= x) + 1\n",
    "\t\tv = x\n",
    "\telse:\n",
    "\t\t# x is number of components.\n",
    "\t\tpca = PCA(n_components=x)\n",
    "\t\tpca.fit(X_train_scaled)\n",
    "\t\tc = x\n",
    "\t\tv = sum(pca.explained_variance_ratio_)\n",
    "\t# Transform train and test datasets.\n",
    "\tX_train_pca = pca.transform(X_train_scaled)\n",
    "\tX_test_pca = pca.transform(X_test_scaled)\n",
    "\treturn X_train_pca, X_test_pca, c, v\n",
    "\n",
    "def train_SKLknn(X_train, y_train, k=5):\n",
    "\tmodel = KNeighborsClassifier(n_neighbors=k)\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\treturn model\n",
    "\n",
    "def predict_SKLknn(model, X): return model.predict(X)\n",
    "\n",
    "# Train model, make model predictions, and save model predictions to CSV file.\n",
    "def generate_predictions_pcaknn(x):\n",
    "\tstart_time = time.time()\n",
    "\tX_train_pca, X_test_pca, c, v = apply_pca(x)\n",
    "\tmodel = train_SKLknn(X_train_pca, y_train, k=2)\n",
    "\tpredictions = predict_SKLknn(model, X_test_pca)\n",
    "\toutput_dir = \"./predictions/pcaknn/\"\n",
    "\tos.makedirs(output_dir, exist_ok=True)\n",
    "\tfile_name = os.path.join(output_dir, f\"predictions.csv\")\n",
    "\tpd.DataFrame({\"id\": S_test[\"id\"], \"label\": predictions}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions_pcaknn(100)\n",
    "# generate_predictions_pcaknn(500)\n",
    "# generate_predictions_pcaknn(1000)\n",
    "# generate_predictions_pcaknn(2000)\n",
    "# generate_predictions_pcaknn(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "- We implemented the models listed here, and some others (which are no longer in this notebook), including Scikit-Learn Gradient Boosting Classifier (SKLgb), Scikit-Learn K-Neighbors Classifier (SKLknn), Scikit-Learn Support Vector Classifier (SKLsvm), Scikit-Learn Random Forest Classifier (SKLrf), and Scikit-Learn Stochastic Gradient Descent Classifier (SKLsgd).\n",
    "- Specific model parameters tried and performance metrics are viewable via the links above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \tn_estimators=100,\n",
    "# \tcriterion=\"gini\",\n",
    "# \tmax_depth=None,\n",
    "# \tmin_samples_split=2,\n",
    "# \tmin_samples_leaf=1,\n",
    "# \tmin_weight_fraction_leaf=0.0,\n",
    "# \tmax_features=\"sqrt\",\n",
    "# \tmax_leaf_nodes=None,\n",
    "# \tmin_impurity_decrease=0.0,\n",
    "# \tbootstrap=False,\n",
    "# \trandom_state=None,\n",
    "# \tclass_weight=None,\n",
    "# \tccp_alpha=0.0,\n",
    "# \tmax_samples=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLlsvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \t\"SKLlsvm\",\n",
    "# \tpenalty=\"l2\",\n",
    "# \tloss=\"squared_hinge\",\n",
    "# \ttol=0.0001,\n",
    "# \tdual=\"auto\",\n",
    "# \tC=1.0,\n",
    "# \trandom_state=None,\n",
    "# \tmax_iter=1000\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLmnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \t\"SKLmnb\",\n",
    "# \talpha=1.0,\n",
    "# \tfit_prior=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLlogreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \t\"SKLlogreg\",\n",
    "# \tC=1.0,\n",
    "# \tmax_iter=100\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLhgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \t\"SKLhgb\",\n",
    "# \tlearning_rate=0.1,\n",
    "# \tmax_iter=100,\n",
    "# \tmax_leaf_nodes=31,\n",
    "# \tmax_depth=None,\n",
    "# \tmin_samples_leaf=20,\n",
    "# \tl2_regularization=0.0,\n",
    "# \trandom_state=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \t\"XGBgb\",\n",
    "# \tn_estimators=100,\n",
    "# \tmax_depth=6,\n",
    "# \tlearning_rate=0.3,\n",
    "# \tbooster=\"gbtree\",\n",
    "# \tsubsample=1.0,\n",
    "# \treg_alpha=0,\n",
    "# \treg_lambda=1,\n",
    "# \trandom_state=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \t\"CBgb\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLstack\n",
    "After cross-validation for tuning hyperparameters, we stacked various combinations of our best performing models together (with a generic LogisticRegression as the final/meta model each time):\n",
    "1. SKLlogreg with `(C=0.64, class_weight=\"balanced\")`\n",
    "2. SKLlsvm with `(penalty=\"l2\", loss=\"hinge\", dual=True, tol=1e-7, C=0.3425, class_weight=\"balanced\", max_iter=5000)`\n",
    "3. CBgb with `(iterations=900, learning_rate=0.1, depth=6, rsm=0.8, auto_class_weights=\"Balanced\", random_strength=1.2, bootstrap_type=\"MVS\", subsample=0.8)`\n",
    "4. SKLet with `(n_estimators=150, max_depth=600, min_samples_split=160, min_impurity_decrease=0.00001, bootstrap=True, class_weight=\"balanced_subsample\", ccp_alpha=0.00001, max_samples=0.9)`\n",
    "5. SKLmnb with `(alpha=1.38, fit_prior=False)`\n",
    "\n",
    "Each stacked combination would generate its own predictions, and the predictions would be randomly combined to form the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \t\"SKLstack\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(int.from_bytes(os.urandom(8), \"big\"))\n",
    "# input_dir = \"predictions/SKLstack\"\n",
    "# input_paths = glob.glob(f\"{input_dir}/*.csv\")\n",
    "# dfs = [pd.read_csv(file) for file in input_paths]\n",
    "# final_predictions = []\n",
    "# for i in range(len(dfs[0])):\n",
    "# \tpredictions = [df.loc[i, \"label\"] for df in dfs]\n",
    "# \tfinal_predictions.append(np.random.choice(predictions))\n",
    "# pd.DataFrame({\"id\": dfs[0][\"id\"], \"label\": final_predictions}).to_csv(\"final_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
