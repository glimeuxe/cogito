{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base import *\n",
    "\n",
    "# Import for cross-validation.\n",
    "from grid_search import cv_grid_search\n",
    "from simulated_annealing import SimulatedAnnealing, cv_simulated_annealing\n",
    "\n",
    "logging.basicConfig(\n",
    "\tfilename=\"main.log\",\n",
    "\tlevel=logging.INFO,\n",
    "\tformat=\"%(asctime)s - %(message)s\",\n",
    "\tdatefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "def train_model(model_type, X_train, y_train, **kwargs):\n",
    "\tmodel_class, default_params = MODEL_TYPE_TO_CLASS_TO_HYPERPARAMETER_MAP[model_type]\n",
    "\tparams = {**default_params, **kwargs}\n",
    "\tmodel = model_class(**params)\n",
    "\tmodel.fit(X_train, y_train, plot=True)\n",
    "\treturn model\n",
    "\n",
    "def predict_model(model, X):\n",
    "\treturn model.predict(X)\n",
    "\n",
    "def generate_predictions(model_type, **kwargs):\n",
    "\tstart_time = time.time()\n",
    "\tmodel = train_model(model_type, np.array(X_train), np.array(y_train), **kwargs)\n",
    "\tpredictions = predict_model(model, np.array(X_test))\n",
    "\toutput_dir = f\"./predictions/{model_type}/\"\n",
    "\tos.makedirs(output_dir, exist_ok=True)\n",
    "\tfile_name = os.path.join(output_dir, f\"{kwargs}.csv\")\n",
    "\tpd.DataFrame({\"id\": S_test[\"id\"], \"label\": predictions}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logreg Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def σ(z): return 1 / (1 + np.exp(-z))\n",
    "def bce_loss(y, ŷ): return (-1/(y.shape[0])) * np.sum(y * np.log(ŷ) + (1 - y) * np.log(1 - ŷ))\n",
    "\n",
    "# Return dw and db, for some X, y, ŷ, w, R, and λ.\n",
    "def gradients_logreg(X, y, ŷ, w, R=None, λ=0):\n",
    "\tm, _ = X.shape\n",
    "\tdw = 1/m * np.dot(X.T, (ŷ - y))\n",
    "\tdb = 1/m * np.sum(ŷ - y)\n",
    "\tif R == \"L2\":\n",
    "\t\tdw += λ * w / m\n",
    "\telif R == \"L1\":\n",
    "\t\tdw += λ * np.sign(w) / m\n",
    "\treturn dw, db\n",
    "\n",
    "# Return (w, b) from gradient descent on X_train and y_train, for some τ, α, G, β, R, and λ.\n",
    "def train_logreg(X_train, y_train, τ=1000, α=0.1, G=\"mini-batch\", β=128, R=None, λ=0):\n",
    "\tm, n = X_train.shape\n",
    "\tw, b = np.zeros((n, 1)), 0\n",
    "\tfor epoch in range(τ):\n",
    "\t\tif G == \"full-batch\":\n",
    "\t\t\tX_batch, y_batch = X_train, y_train\n",
    "\t\t\tŷ = σ(np.dot(X_batch, w) + b)\n",
    "\t\t\tdw, db = gradients_logreg(X_batch, y_batch, ŷ, w, R, λ)\n",
    "\t\t\tw, b = w - α*dw, b - α*db\n",
    "\t\telif G == \"mini-batch\":\n",
    "\t\t\tfor i in range(0, m, β):\n",
    "\t\t\t\tX_batch, y_batch = X_train[i:i+β], y_train[i:i+β]\n",
    "\t\t\t\tŷ = σ(np.dot(X_batch, w) + b)\n",
    "\t\t\t\tdw, db = gradients_logreg(X_batch, y_batch, ŷ, w, R, λ)\n",
    "\t\t\t\tw, b = w - α*dw, b - α*db\n",
    "\t\telif G == \"stochastic\":\n",
    "\t\t\tfor i in range(m):\n",
    "\t\t\t\tX_batch, y_batch = X_train[i:i+1], y_train[i:i+1]\n",
    "\t\t\t\tŷ = σ(np.dot(X_batch, w) + b)\n",
    "\t\t\t\tdw, db = gradients_logreg(X_batch, y_batch, ŷ, w, R, λ)\n",
    "\t\t\t\tw, b = w - α*dw, b - α*db\n",
    "\treturn w, b\n",
    "\n",
    "# Return array of predictions, where each prediction is 1 if corresponding ŷ entry > 0.5, and 0 otherwise.\n",
    "def predict_logreg(wb_tuple, X):\n",
    "\tw, b = wb_tuple\n",
    "\tŷ = σ(np.dot(X, w) + b)\n",
    "\treturn np.array([1 if p > 0.5 else 0 for p in ŷ])\n",
    "\n",
    "# Train model, make predictions, and save predictions to CSV file.\n",
    "def generate_predictions_logreg(τ=1000, α=0.1, G=\"mini-batch\", β=128, R=None, λ=0):\n",
    "\tstart_time = time.time()\n",
    "\tw, b = train_logreg(np.array(X_train), np.array(y_train), τ, α, G, β, R, λ)\n",
    "\tpredictions = predict_logreg((w, b), np.array(X_test))\n",
    "\tos.makedirs(\"./predictions/logreg/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/logreg/\", f\"τ={τ},α={α},G={G},β={β},R={R},λ={λ}.csv\")\n",
    "\tpd.DataFrame({\"id\": S_test[\"id\"], \"label\": predictions}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(x):\n",
    "\tscaler = StandardScaler()\n",
    "\tX_train_scaled = scaler.fit_transform(X_train)\n",
    "\tX_test_scaled = scaler.transform(X_test)\n",
    "\tif 0 <= x <= 1:\n",
    "\t\t# x is variance threshold.\n",
    "\t\tpca = PCA(n_components=None)\n",
    "\t\tpca.fit(X_train_scaled)\n",
    "\t\tc = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= x) + 1\n",
    "\t\tv = x\n",
    "\telse:\n",
    "\t\t# x is number of components.\n",
    "\t\tpca = PCA(n_components=x)\n",
    "\t\tpca.fit(X_train_scaled)\n",
    "\t\tc = x\n",
    "\t\tv = sum(pca.explained_variance_ratio_)\n",
    "\t# Transform train and test datasets.\n",
    "\tX_train_pca = pca.transform(X_train_scaled)\n",
    "\tX_test_pca = pca.transform(X_test_scaled)\n",
    "\treturn X_train_pca, X_test_pca, c, v\n",
    "\n",
    "def train_SKLknn(X_train, y_train, k=5, W=\"uniform\", p=2, m=\"minkowski\"):\n",
    "\tmodel = KNeighborsClassifier(\n",
    "\t\tn_neighbors=k,\n",
    "\t\tweights=W,\n",
    "\t\tp=p,\n",
    "\t\tmetric=m,\n",
    "\t\tn_jobs=-1\n",
    "\t)\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\treturn model\n",
    "\n",
    "def predict_SKLknn(model, X): return model.predict(X)\n",
    "\n",
    "# Train model, make model predictions, and save model predictions to CSV file.\n",
    "def generate_predictions_pcaknn(x):\n",
    "\tstart_time = time.time()\n",
    "\tX_train_pca, X_test_pca, c, v = apply_pca(x)\n",
    "\tmodel = train_SKLknn(X_train_pca, y_train, k=2)\n",
    "\tpredictions = predict_SKLknn(model, X_test_pca)\n",
    "\tos.makedirs(\"./predictions/pcaknn/\", exist_ok=True)\n",
    "\tfile_name = os.path.join(\"./predictions/pcaknn/\", f\"pcaknn(ρ={c},ν={v:.2f}).csv\")\n",
    "\tpd.DataFrame({\"id\": S_test[\"id\"], \"label\": predictions}).to_csv(file_name, index=False)\n",
    "\tend_time = time.time()\n",
    "\tlogging.info(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")\n",
    "\tprint(f\"Predictions file {file_name} generated in {end_time - start_time:.2f}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pcaknn Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_predictions_pcaknn(100)\n",
    "generate_predictions_pcaknn(500)\n",
    "generate_predictions_pcaknn(1000)\n",
    "generate_predictions_pcaknn(2000)\n",
    "generate_predictions_pcaknn(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLknn Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \t\"SKLknn\",\n",
    "# \tn_neighbors=7,\n",
    "# \tweights=\"distance\",\n",
    "# \tp=1,\n",
    "# \tmetric=\"minkowski\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLrf Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \t\"SKLrf\",\n",
    "# \tn_estimators=150,\n",
    "# \tcriterion=\"entropy\",\n",
    "# \tmax_depth=10,\n",
    "# \tmin_samples_split=5,\n",
    "# \tmin_samples_leaf=2,\n",
    "# \tmax_leaf_nodes=50,\n",
    "# \trandom_state=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLsvm Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \t\"SKLsvm\",\n",
    "# \tC=0.5,\n",
    "# \tkernel=\"linear\",\n",
    "# \tdegree=3,\n",
    "# \tgamma=\"auto\",\n",
    "# \tmax_iter=1000,\n",
    "# \trandom_state=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLgb Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \t\"SKLgb\",\n",
    "# \tlearning_rate=0.05,\n",
    "# \tn_estimators=200,\n",
    "# \tsubsample=0.8,\n",
    "# \tmin_samples_split=3,\n",
    "# \tmin_samples_leaf=2,\n",
    "# \tmax_depth=5,\n",
    "# \trandom_state=None,\n",
    "# \tmax_leaf_nodes=50\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLhgb Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \t\"SKLhgb\",\n",
    "# \tlearning_rate=0.01,\n",
    "# \tmax_iter=200,\n",
    "# \tmax_leaf_nodes=50,\n",
    "# \tmax_depth=5,\n",
    "# \tmin_samples_leaf=10,\n",
    "# \trandom_state=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBgb Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \t\"XGBgb\",\n",
    "# \tn_estimators=150,\n",
    "# \tmax_depth=4,\n",
    "# \tmax_leaves=15,\n",
    "# \tlearning_rate=0.05,\n",
    "# \tobjective=\"binary:logistic\",\n",
    "# \tbooster=\"gbtree\",\n",
    "# \tsubsample=0.9,\n",
    "# \treg_alpha=0.1,\n",
    "# \treg_lambda=1.2,\n",
    "# \trandom_state=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBgb Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_predictions(\n",
    "# \t\"CBgb\",\n",
    "# \tlearning_rate=0.03,\n",
    "# \tsubsample=0.9,\n",
    "# \tmax_depth=6,\n",
    "# \tn_estimators=1000,\n",
    "# \treg_lambda=1.0,\n",
    "# \trandom_state=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "\t\"learning_rate\": [0.08, 0.09, 0.1],\n",
    "\t\"max_depth\": [8, 9],\n",
    "\t\"n_estimators\": [500]\n",
    "}\n",
    "cv_grid_search(\"CBgb\", grid, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = {\n",
    "# \t\"learning_rate\": (0.06, 0.12),\n",
    "# \t\"max_depth\": [12],\n",
    "# \t\"reg_lambda\": (0.1, 1),\n",
    "# }\n",
    "\n",
    "# cv_simulated_annealing(model_name=\"CBgb\", grid=grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
